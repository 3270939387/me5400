# 基于图像的机械臂视觉伺服训练计划

## 📋 任务定义

### 目标
训练一个**完全由图像驱动的机械臂控制策略**，能够从任意随机初始关节位置出发，通过观察相机图像，自主运动到能够清晰捕捉 marker 的位置。

### 任务形式化
- **观测 (Observation)**: D405 相机 RGB 图像 (1920×1080 或降采样到 640×480/320×240)
- **动作 (Action)**: 7 维关节位置增量 `Δq` 或关节速度 `q̇`
- **目标 (Goal)**: 
  - Marker 出现在相机视野内
  - Marker 接近图像中心（像素坐标接近图像中心）
  - Marker 尺寸合适（不太远/太近）

### 成功标准
- **定性**: 视觉上 marker 清晰可见，位于画面中心附近
- **定量**: 
  - Marker 像素坐标到图像中心的距离 < 阈值（如 50 像素）
  - Marker 在视野内的时间占比 > 80%
  - 动作平滑，无剧烈抖动

---

## 📦 阶段 1: 数据收集与格式设计

### 1.1 数据生成流程
**当前状态**: `lula.py` 已实现
- ✅ 随机初始关节角
- ✅ RMPflow expert policy 生成轨迹
- ✅ 相机图像采集
- ⚠️ **待添加**: 元数据收集与保存

### 1.2 数据格式设计

#### 目录结构
```
dataset_root/
├── episodes/
│   ├── episode_0001/
│   │   ├── rgb_0000.png
│   │   ├── rgb_0001.png
│   │   ├── ...
│   │   └── meta.json
│   ├── episode_0002/
│   │   └── ...
│   └── ...
└── index.json  (可选，全局索引)
```

#### `meta.json` 结构
```json
{
  "episode_id": "episode_0001",
  "num_steps": 120,
  "env_path": "/home/alphatok/ME5400/env.setup/env.usda",
  "cam_path": "/World/Panda/D405_rigid/D405/Camera_OmniVision_OV9782_Color",
  "marker_path": "/World/Phantom/marker",
  "initial_joint_positions": [0.12, -0.85, 0.30, -1.90, 0.05, 1.80, -0.40],
  "steps": [
    {
      "step": 0,
      "image": "rgb_0000.png",
      "joint_positions": [0.12, -0.85, 0.30, -1.90, 0.05, 1.80, -0.40],
      "joint_velocities": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
      "action": {
        "type": "joint_position_delta",
        "delta_q": [0.01, -0.02, 0.00, 0.03, -0.01, 0.00, 0.00]
      },
      "marker_pixel": {
        "u": 623.5,
        "v": 310.2,
        "visible": true,
        "distance_to_center": 45.3
      },
      "done": false
    }
  ]
}
```

#### 关键字段说明
- `joint_positions`: 当前 7 维关节角 (用于状态监督/辅助输入)
- `action.delta_q`: RMPflow 输出的关节位置增量 (模仿学习目标)
- `marker_pixel`: 
  - `u, v`: marker 在图像中的像素坐标（通过 3D→2D 投影计算）
  - `visible`: 是否在视野内
  - `distance_to_center`: 到图像中心的像素距离（用于奖励设计）

### 1.3 数据收集改进点
- [ ] **添加 marker 像素坐标计算**: 使用 Isaac Sim 的相机投影 API 或手动 3D→2D 投影
- [ ] **保存 RMPflow 动作**: 在 `controller.update()` 后获取 `action` 并保存
- [ ] **添加终止条件判断**: 当 marker 接近中心且稳定时，标记 `done=True`
- [ ] **数据增强准备**: 考虑添加深度图、分割掩码（可选）

---

## 🏗️ 阶段 2: 模型架构设计

### 2.1 推荐架构（两阶段）

#### 方案 A: 端到端策略网络（推荐起步）
```
输入: RGB 图像 (H×W×3)
  ↓
视觉编码器 (ResNet18/34 或 ViT-small, ImageNet 预训练)
  ↓
特征向量 z_t (512-dim)
  ↓
策略头 (MLP: 512 → 256 → 128 → 7)
  ↓
输出: 关节位置增量 Δq (7-dim)
```

**优点**: 简单直接，适合 Behavior Cloning  
**缺点**: 如果图像质量不稳定，可能难以收敛

#### 方案 B: 感知 + 控制分离（更稳健）
```
阶段 1: 感知网络
  输入: RGB 图像
  ↓
  ResNet18 (backbone)
  ↓
  输出: marker 像素坐标 (u, v) 或 heatmap (H×W)

阶段 2: 控制策略
  输入: 图像特征 z_t + (可选) 当前关节角 q_t
  ↓
  MLP
  ↓
  输出: Δq
```

**优点**: 
- 可以单独验证感知模块
- 控制策略可以基于更稳定的特征
- 便于调试

**缺点**: 需要两阶段训练，实现稍复杂

### 2.2 模型选择建议

#### 视觉编码器
- **ResNet18/34**: 轻量，适合实时控制，ImageNet 预训练权重
- **ViT-small**: 如果数据量大，可以尝试，但可能过拟合
- **MobileNetV2**: 如果追求极低延迟

#### 策略头
- **简单 MLP**: 2-3 层，隐藏层 256/128
- **可选**: 添加 BatchNorm / Dropout（但 BC 通常不需要）

### 2.3 输入/输出设计

#### 输入预处理
- 图像: `(H, W, 3) → (224, 224, 3)` 或保持原分辨率
- 归一化: ImageNet 标准归一化 `(x - mean) / std`
- 可选: 添加当前关节角 `q_t` 作为额外输入（concatenate 到特征向量）

#### 输出后处理
- `Δq` 直接输出（单位: 弧度）
- 可选: 添加 tanh 激活 + 缩放，限制单步最大变化量
- 可选: 添加关节限制检查（clamp 到安全范围）

---

## 🎓 阶段 3: 训练策略

### 3.1 Behavior Cloning (BC) / Imitation Learning

#### 损失函数
```python
# 方案 1: L2 损失（简单）
loss = || Δq_pred - Δq_expert ||²

# 方案 2: L1 损失（对异常值更鲁棒）
loss = || Δq_pred - Δq_expert ||₁

# 方案 3: Huber 损失（结合 L1/L2 优点）
loss = Huber(Δq_pred, Δq_expert, delta=0.1)
```

#### 训练配置
- **优化器**: Adam (lr=1e-4 或 3e-4)
- **学习率调度**: CosineAnnealingLR 或 ReduceLROnPlateau
- **批次大小**: 32-64（取决于 GPU 内存）
- **训练/验证分割**: 80/20 或 90/10
- **数据增强**（可选）:
  - 随机裁剪/翻转（但要小心，可能改变 marker 位置语义）
  - 颜色抖动（亮度/对比度）
  - 高斯噪声

### 3.2 两阶段训练（如果采用方案 B）

#### 阶段 1: 感知网络
- **任务**: 从 RGB 图像预测 marker 像素坐标 `(u, v)`
- **损失**: L2 或 Smooth L1
- **评估**: 像素坐标误差（MAE）

#### 阶段 2: 控制策略
- **任务**: 从图像特征（+ 可选关节角）预测 `Δq`
- **损失**: 同 BC
- **可选**: 冻结感知网络，只训练策略头

### 3.3 潜在改进: RL 微调（可选，阶段 4）

如果 BC 效果不够好（抖动、不稳定），可以考虑：

- **算法**: PPO 或 SAC
- **奖励设计**:
  ```python
  reward = -distance_to_center_penalty 
           - action_magnitude_penalty 
           + success_bonus
  ```
- **初始化**: 用 BC 模型权重初始化 policy
- **环境**: 在 Isaac Sim 中直接运行（需要封装成 Gym 接口）

---

## 📊 阶段 4: 评估与测试

### 4.1 离线评估指标

#### 动作预测误差
- **MAE**: 平均绝对误差 `|Δq_pred - Δq_expert|`
- **MSE**: 均方误差
- **逐关节误差**: 检查哪个关节预测最不准

#### 感知评估（如果采用方案 B）
- **像素坐标误差**: `|(u_pred, v_pred) - (u_true, v_true)|`
- **可见性分类准确率**: marker 是否在视野内的二分类

### 4.2 在线评估（Isaac Sim 闭环测试）

#### 评估脚本: `lula_eval.py`
- 加载训练好的模型
- 随机初始关节角（与训练时一致）
- **不使用 RMPflow**，只用学习到的策略
- 记录:
  - 成功率（marker 是否到达中心）
  - 平均步数
  - 动作平滑度（加速度/加加速度）
  - 失败案例（碰撞、超限等）

#### 评估指标
- **成功率**: `success_rate = num_success / num_episodes`
- **平均步数**: 到达目标所需的平均仿真步数
- **轨迹平滑度**: 动作变化的标准差
- **鲁棒性**: 不同初始位置下的成功率分布

---

## 🚀 实现步骤规划

### Step 1: 完善数据收集 (`lula.py` 改进)
- [ ] 添加 marker 像素坐标计算（3D→2D 投影）
- [ ] 保存每步的 `joint_positions`、`action`、`marker_pixel` 到 `meta.json`
- [ ] 按 episode 组织数据（每个 episode 一个文件夹）
- [ ] 添加终止条件判断（marker 接近中心且稳定）
- [ ] 生成至少 50-100 个 episodes（用于初步训练）

**预计时间**: 1-2 天

### Step 2: 数据加载与预处理工具
- [ ] 编写 `dataset.py`: 实现 PyTorch `Dataset` 类
- [ ] 图像加载与预处理（resize, normalize）
- [ ] 元数据解析（从 `meta.json` 读取）
- [ ] 数据增强（可选）

**预计时间**: 0.5-1 天

### Step 3: 模型实现
- [ ] 实现视觉编码器（ResNet18 + 预训练权重）
- [ ] 实现策略头（MLP）
- [ ] 端到端模型类 `VisionPolicy`
- [ ] 模型前向传播测试

**预计时间**: 1 天

### Step 4: 训练脚本
- [ ] 实现 BC 训练循环
- [ ] 损失函数、优化器、学习率调度
- [ ] 训练/验证循环
- [ ] 模型保存与加载
- [ ] TensorBoard 日志（可选）

**预计时间**: 1-2 天

### Step 5: 评估脚本 (`lula_eval.py`)
- [ ] 在 Isaac Sim 中加载模型
- [ ] 实现策略推理循环（图像 → 动作）
- [ ] 记录评估指标
- [ ] 可视化轨迹（可选）

**预计时间**: 1-2 天

### Step 6: 迭代优化
- [ ] 分析失败案例
- [ ] 调整模型架构/超参数
- [ ] 收集更多数据（如果需要）
- [ ] 考虑 RL 微调（如果 BC 不够好）

**预计时间**: 持续迭代

---

## 🔍 潜在改进点与风险

### 改进点

1. **数据多样性**
   - 增加不同光照条件
   - 增加 marker 位置变化（不只是固定位置）
   - 增加背景干扰（可选）

2. **模型架构**
   - 考虑使用 **CNN + LSTM** 处理时序信息（如果动作有依赖）
   - 考虑 **注意力机制**（关注 marker 区域）
   - 考虑 **多尺度特征融合**（FPN 风格）

3. **训练策略**
   - **课程学习**: 从简单初始位置开始，逐渐增加难度
   - **数据平衡**: 确保不同初始位置的数据量均衡
   - **在线数据收集**: 训练过程中继续生成新数据（DAgger 风格）

4. **奖励设计**（如果做 RL）
   - 不仅考虑 marker 位置，还考虑图像清晰度
   - 添加动作平滑性奖励
   - 添加避障奖励（如果环境有障碍物）

### 风险与应对

1. **数据分布偏移**
   - **风险**: 训练数据与测试环境不一致
   - **应对**: 确保训练时的随机初始位置覆盖足够广

2. **过拟合**
   - **风险**: 模型只记住特定轨迹，泛化差
   - **应对**: 增加数据量、数据增强、正则化（Dropout/BatchNorm）

3. **动作不稳定**
   - **风险**: 预测的动作抖动，导致机械臂震荡
   - **应对**: 
     - 添加动作平滑（移动平均）
     - 在损失函数中添加动作变化惩罚
     - 使用 LSTM 处理时序

4. **Sim-to-Real 差距**（如果未来要部署到真实机器人）
   - **风险**: 仿真训练的模型在真实环境失效
   - **应对**: 
     - 域随机化（光照、纹理、相机噪声）
     - 真实数据微调
     - 使用更真实的渲染（Path Tracing）

---

## 📝 技术栈建议

### 深度学习框架
- **PyTorch** (推荐): 灵活，适合研究
- **TensorFlow/Keras**: 如果团队更熟悉

### 训练工具
- **PyTorch Lightning**: 简化训练循环（可选）
- **Weights & Biases (wandb)**: 实验跟踪（可选）
- **TensorBoard**: 可视化（PyTorch 内置）

### Isaac Sim 集成
- **omni.isaac.gym**: 如果要做 RL（Isaac Sim 的 Gym 接口）
- **Stable-Baselines3**: RL 算法库（如果做 RL 微调）

---

## 🎯 里程碑检查点

### Milestone 1: 数据收集完成
- [ ] 至少 50 个 episodes，每个 100+ 步
- [ ] 数据格式统一，`meta.json` 完整
- [ ] 数据可视化检查（随机采样几张图，检查 marker 位置标注）

### Milestone 2: 模型训练完成
- [ ] BC 训练收敛（训练/验证损失下降）
- [ ] 离线评估: 动作预测误差 < 阈值（如 MAE < 0.1 rad）
- [ ] 模型保存与加载正常

### Milestone 3: 在线评估通过
- [ ] 成功率 > 60%（初步目标）
- [ ] 动作平滑，无剧烈抖动
- [ ] 失败案例分析完成

### Milestone 4: 优化迭代
- [ ] 成功率 > 80%（理想目标）
- [ ] 泛化测试通过（不同初始位置）
- [ ] 代码文档化，可复现

---

## 📚 参考资源

### 相关论文
- **End-to-End Learning for Visual Servoing**: 端到端视觉伺服
- **Learning to See Before Learning to Act**: 感知+控制分离
- **DAgger**: 在线模仿学习
- **Sim-to-Real Transfer**: 仿真到真实迁移

### 代码库参考
- **pytorch-a2c-ppo-acktr**: PPO 实现
- **stable-baselines3**: RL 算法库
- **Isaac Sim Examples**: 官方示例代码

---

## ❓ 待确认问题

1. **动作空间**: 使用 `Δq`（位置增量）还是 `q̇`（速度）？推荐 `Δq`，更稳定
2. **图像分辨率**: 1920×1080 还是降采样？推荐 640×480 或 320×240，平衡速度与精度
3. **数据量**: 需要多少 episodes？建议至少 100+，理想 500+
4. **评估频率**: 多久评估一次？建议每 10-20 个 episodes 评估一次
5. **硬件要求**: GPU 内存？建议至少 8GB（训练 ResNet18）

---

**最后更新**: 2024-12-19  
**版本**: v1.0

