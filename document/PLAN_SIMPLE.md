# 图像驱动机械臂控制 - 精简计划

## 🎯 任务目标

**训练一个纯视觉驱动的机械臂策略**：从随机初始关节位置出发，仅通过观察相机图像，自主运动到能够清晰捕捉 marker 的位置。

- **输入**: D405 相机 RGB 图像
- **输出**: 7 维关节位置增量 `Δq`
- **成功标准**: Marker 出现在图像中心附近（像素距离 < 50px）

---

## 📋 实现步骤

### Step 1: 数据收集（完善 `lula.py`）
- [ ] 计算 marker 像素坐标（3D→2D 投影）
- [ ] 保存每步数据到 `meta.json`：
  - 图像路径
  - 当前关节角 `q_t`
  - RMPflow 动作 `Δq`（模仿目标）
  - Marker 像素坐标 `(u, v)`
- [ ] 按 episode 组织：`episodes/episode_0001/`
- [ ] 生成 100+ episodes（每个 100+ 步）

### Step 2: 模型实现
- [ ] **架构**: ResNet18 (ImageNet 预训练) → MLP (512→256→128→7)
- [ ] **输入**: RGB 图像 (640×480 或 320×240)
- [ ] **输出**: 关节位置增量 `Δq` (7-dim)

### Step 3: 训练（Behavior Cloning）
- [ ] **损失**: L2 损失 `|| Δq_pred - Δq_expert ||²`
- [ ] **优化器**: Adam (lr=1e-4)
- [ ] **批次**: 32-64
- [ ] **数据分割**: 80% 训练 / 20% 验证

### Step 4: 评估（Isaac Sim 闭环测试）
- [ ] 实现 `lula_eval.py`：加载模型，不使用 RMPflow
- [ ] 随机初始位置测试
- [ ] 记录成功率、平均步数、动作平滑度

### Step 5: 迭代优化
- [ ] 分析失败案例
- [ ] 调整超参数/架构
- [ ] 如需要，考虑 RL 微调（PPO/SAC）

---

## 📦 数据格式

```
dataset_root/
├── episodes/
│   ├── episode_0001/
│   │   ├── rgb_0000.png
│   │   ├── rgb_0001.png
│   │   └── meta.json
│   └── ...
```

**`meta.json` 示例**:
```json
{
  "episode_id": "episode_0001",
  "num_steps": 120,
  "steps": [
    {
      "step": 0,
      "image": "rgb_0000.png",
      "joint_positions": [0.12, -0.85, 0.30, -1.90, 0.05, 1.80, -0.40],
      "action": {"delta_q": [0.01, -0.02, 0.00, 0.03, -0.01, 0.00, 0.00]},
      "marker_pixel": {"u": 623.5, "v": 310.2, "visible": true}
    }
  ]
}
```

---

## 🏗️ 模型架构

```
RGB Image (H×W×3)
    ↓
ResNet18 (ImageNet 预训练)
    ↓
特征向量 (512-dim)
    ↓
MLP (512 → 256 → 128 → 7)
    ↓
Δq (7-dim 关节位置增量)
```

---

## 📊 评估指标

- **成功率**: marker 到达中心的比例
- **平均步数**: 到达目标所需步数
- **动作平滑度**: 动作变化的标准差
- **离线误差**: `MAE(Δq_pred, Δq_expert)`

---

## ⚠️ 关键风险

1. **过拟合**: 增加数据量、数据增强
2. **动作抖动**: 添加动作平滑（移动平均）
3. **数据分布偏移**: 确保随机初始位置覆盖广

---

## 🎯 里程碑

- [ ] **M1**: 数据收集完成（100+ episodes）
- [ ] **M2**: 模型训练收敛（验证损失下降）
- [ ] **M3**: 在线评估成功率 > 60%
- [ ] **M4**: 优化后成功率 > 80%

