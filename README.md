利用 Isaac Sim 搭建的手术场景（Panda 机械臂 + Vention 手术台 + ABD Phantom + D405 相机），采集从机械臂随机初始姿态出发的相机图像轨迹，并基于这些数据训练一个“纯视觉驱动”的控制策略，使机械臂仅凭相机图像就能移动到相机能清晰看到 phantom 上 marker 的位置。
具体包括三部分工作：
数据采集：在 Isaac Sim 中使用 RMPflow 作为 expert，从大量随机初始关节角出发，让机械臂运动到靠近 marker 的位置，并记录每一步的相机图像、关节状态、RMPflow 输出动作以及 marker 在图像中的像素坐标。
模型训练：使用采集到的图像和 expert 动作，通过 Behavior Cloning 训练一个基于 ResNet18 的端到端视觉策略网络，实现从单帧图像到关节增量 Δq 的映射。
策略评估：在 Isaac Sim 中加载训练好的模型，在不同随机初始姿态下闭环控制机械臂，评估其让相机对准 marker 的成功率、收敛速度和动作平滑性。
