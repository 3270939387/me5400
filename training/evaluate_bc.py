#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Behavior Cloning æ¨¡å‹è¯„ä¼°è„šæœ¬
åœ¨Isaac Simä¸­åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½

è¿è¡Œæ–¹å¼ï¼ˆå¿…é¡»ä½¿ç”¨Isaac Simçš„Pythonï¼‰:
    ~/isaacsim/python.sh /home/alphatok/ME5400/training/evaluate_bc.py --checkpoint <path> --num_episodes 20

æˆ–è€…ä½¿ç”¨æä¾›çš„å¯åŠ¨è„šæœ¬:
    bash run_evaluate.sh <checkpoint_path> [num_episodes]
"""

from isaacsim import SimulationApp

# å¯åŠ¨ Isaac Sim
simulation_app = SimulationApp({"headless": False})

import os
import sys
import time
import json
import argparse
import numpy as np
import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms
from PIL import Image

import omni.timeline
import omni.usd
from pxr import UsdPhysics, Gf, Usd

# æ ¸å¿ƒæ¨¡å—
from omni.isaac.core import SimulationContext
from omni.isaac.core.utils.types import ArticulationAction
from isaacsim.core.prims import SingleArticulation as Articulation
from isaacsim.core.prims import SingleXFormPrim as XFormPrim
from omni.kit.viewport.utility import get_active_viewport, capture_viewport_to_file

# ===================== æ¨¡å‹å®šä¹‰ =====================

class ResNetMLPPolicy(nn.Module):
    """ResNet18 è§†è§‰ç¼–ç  + MLP åŠ¨ä½œå¤´ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    def __init__(self, out_dim=7):
        super().__init__()
        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # [B,512,1,1]
        self.head = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, out_dim),
        )

    def forward(self, x):
        feat = self.backbone(x).flatten(1)  # [B,512]
        return self.head(feat)              # [B,7]

# ===================== é…ç½® =====================

ENV_USD_PATH = "/home/alphatok/ME5400/env.setup/env.usda"
MARKER_PATH = "/World/Phantom/marker"
ROBOT_PATH = "/World/Panda"
TABLE_PATH = "/World/Table"
CAM_PATH = "/World/Panda/D405_rigid/D405/Camera_OmniVision_OV9782_Color"
TCP_PATH = "/World/Panda/TCP"

DT = 1.0 / 60.0

# Panda å…³èŠ‚é™åˆ¶
PANDA_JOINT_LIMITS = [
    (-2.8973, 2.8973),   # joint1
    (-1.7628, 1.7628),   # joint2
    (-2.8973, 2.8973),   # joint3
    (-3.0718, -0.0698),  # joint4
    (-2.8973, 2.8973),   # joint5
    (-0.0175, 3.7525),   # joint6
    (-2.8973, 2.8973),   # joint7
]

# å·¥ä½œç©ºé—´å®šä¹‰ï¼ˆä¸æ•°æ®æ”¶é›†æ—¶ä¸€è‡´ï¼‰
WORKSPACE_CENTER = np.array([0.0, 0.50, 0.50])  # ç±³
WORKSPACE_RADIUS = 0.25  # ç±³ï¼ˆ25cmï¼‰
WORKSPACE_Z_MIN = 0.20  # ç±³
WORKSPACE_Z_MAX = 0.75  # ç±³

# æˆåŠŸæ¡ä»¶ï¼ˆä¸æ•°æ®æ”¶é›†æ—¶ä¸€è‡´ï¼‰
SUCCESS_DISTANCE_X_MAX = 0.1   # ç±³
SUCCESS_DISTANCE_Y_MAX = 0.1   # ç±³
SUCCESS_DISTANCE_Z_MAX = 0.3   # ç±³

# ç¢°æ’æ£€æµ‹é˜ˆå€¼
COLLISION_VELOCITY_THRESHOLD = 10.0  # rad/s
COLLISION_ACCELERATION_THRESHOLD = 50.0  # rad/sÂ²

# ===================== è¾…åŠ©å‡½æ•° =====================

class ViewportCamera:
    """è§†å£ç›¸æœºå°è£…"""
    def __init__(self, camera_path, resolution=(1280, 720)):
        self.viewport_api = get_active_viewport()
        if not self.viewport_api:
            raise RuntimeError("âŒ æ— æ³•æ‰¾åˆ°æ´»è·ƒè§†å£ï¼")
        self.viewport_api.camera_path = camera_path
        self.viewport_api.set_texture_resolution(resolution)

    def capture(self, filename):
        try:
            capture_viewport_to_file(self.viewport_api, filename)
            return True
        except Exception as e:
            print(f"âŒ æˆªå›¾å¼‚å¸¸: {e}")
            return False

def sample_random_joint_config(num_joints):
    """åœ¨å…³èŠ‚é™ä½å†…éšæœºé‡‡æ ·å…³èŠ‚é…ç½®"""
    random_joint_positions = []
    for i in range(num_joints):
        if i < len(PANDA_JOINT_LIMITS):
            lower, upper = PANDA_JOINT_LIMITS[i]
            random_joint_positions.append(np.random.uniform(lower, upper))
        else:
            random_joint_positions.append(np.random.uniform(-np.pi, np.pi))
    return np.array(random_joint_positions, dtype=np.float32)

def check_workspace_constraint(ee_pos_base):
    """
    æ£€æŸ¥æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®æ˜¯å¦åœ¨å·¥ä½œç©ºé—´å†…
    ee_pos_base: æœ«ç«¯æ‰§è¡Œå™¨åœ¨ Panda base åæ ‡ç³»ä¸‹çš„ä½ç½® (x, y, z)
    è¿”å›: (is_valid, reason)
    """
    # 1. æ£€æŸ¥çƒçº¦æŸï¼š||p_ee - center|| <= radius
    offset = ee_pos_base - WORKSPACE_CENTER
    distance = np.linalg.norm(offset)
    if distance > WORKSPACE_RADIUS:
        return False, f"è¶…å‡ºçƒåŠå¾„: {distance:.3f}m > {WORKSPACE_RADIUS}m"
    
    # 2. æ£€æŸ¥ZèŒƒå›´çº¦æŸï¼šz_min <= z <= z_max
    z = ee_pos_base[2]
    if z < WORKSPACE_Z_MIN:
        return False, f"Zè¿‡ä½: {z:.3f}m < {WORKSPACE_Z_MIN}m"
    if z > WORKSPACE_Z_MAX:
        return False, f"Zè¿‡é«˜: {z:.3f}m > {WORKSPACE_Z_MAX}m"
    
    return True, "OK"

def sample_valid_initial_config(robot, sim, max_attempts=100):
    """
    ä½¿ç”¨æ‹’ç»é‡‡æ ·æ‰¾åˆ°å·¥ä½œç©ºé—´å†…çš„æœ‰æ•ˆåˆå§‹é…ç½®ï¼ˆä¸æ•°æ®æ”¶é›†æ—¶ä¸€è‡´ï¼‰
    è¿”å›: (joint_positions, ee_pos_base) æˆ– (None, None) å¦‚æœå¤±è´¥
    """
    # è·å– Panda base çš„ä¸–ç•Œå˜æ¢
    base_quat = None
    base_pos = None
    
    try:
        base_prim = XFormPrim("/World/Panda")
        base_world_pos, base_world_orn = base_prim.get_world_pose()
        
        # ç¡®ä¿è½¬æ¢ä¸º Python float
        base_pos = [float(base_world_pos[0]), float(base_world_pos[1]), float(base_world_pos[2])]
        base_orn = [float(base_world_orn[0]), float(base_world_orn[1]), float(base_world_orn[2]), float(base_world_orn[3])]
        
        # ä½¿ç”¨ Gf åº“å¤„ç†å››å…ƒæ•°å’Œæ—‹è½¬
        base_quat = Gf.Quatd(float(base_orn[0]), Gf.Vec3d(float(base_orn[1]), float(base_orn[2]), float(base_orn[3])))
        
    except Exception as e:
        # ç®€åŒ–æ–¹æ³•ï¼šåªè€ƒè™‘å¹³ç§»
        try:
            base_pos = [float(base_world_pos[0]), float(base_world_pos[1]), float(base_world_pos[2])]
        except:
            base_pos = [0.0, 0.0, 0.0]
        base_quat = None
    
    # æ„å»ºä»worldåˆ°baseçš„å˜æ¢
    def world_to_base(p_world):
        p_world = np.array([float(p_world[0]), float(p_world[1]), float(p_world[2])], dtype=float)
        p_rel = Gf.Vec3d(p_world[0] - float(base_pos[0]),
                         p_world[1] - float(base_pos[1]),
                         p_world[2] - float(base_pos[2]))
        
        if base_quat is None:
            return np.array([float(p_rel[0]), float(p_rel[1]), float(p_rel[2])], dtype=float)
        
        q_inv = base_quat.GetInverse()
        p_base = q_inv.Transform(p_rel)
        return np.array([float(p_base[0]), float(p_base[1]), float(p_base[2])], dtype=float)
    
    num_joints = robot.num_dof
    
    for attempt in range(max_attempts):
        # 1. éšæœºé‡‡æ ·å…³èŠ‚é…ç½®
        joint_positions = sample_random_joint_config(num_joints)
        
        # 2. è®¾ç½®å…³èŠ‚ä½ç½®ï¼ˆå…ˆé‡ç½®é€Ÿåº¦ï¼Œé¿å…çªç„¶å˜åŒ–ï¼‰
        robot.set_joint_velocities(np.zeros(num_joints))
        robot.set_joint_positions(joint_positions)
        
        # 3. æ¨è¿›æ›´å¤šå¸§è®©ç‰©ç†ç¨³å®š
        for _ in range(10):
            sim.step(render=False)
        
        # 4. è·å–TCPçš„ä¸–ç•Œåæ ‡
        try:
            tcp_prim = XFormPrim(TCP_PATH)
            tcp_world_pos, _ = tcp_prim.get_world_pose()
        except Exception as e:
            continue
        
        # 5. è½¬æ¢åˆ°baseåæ ‡ç³»
        tcp_base_pos = world_to_base(tcp_world_pos)
        
        # 6. æ£€æŸ¥å·¥ä½œç©ºé—´çº¦æŸ
        is_valid, reason = check_workspace_constraint(tcp_base_pos)
        
        if is_valid:
            return joint_positions, tcp_base_pos
        else:
            if attempt < 5 or attempt % 20 == 0:
                pass  # è¯„ä¼°æ—¶ä¸éœ€è¦æ‰“å°å¤ªå¤šä¿¡æ¯
    
    return None, None

def load_model(checkpoint_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = ResNetMLPPolicy(out_dim=7).to(device)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    print(f"âœ… å·²åŠ è½½æ¨¡å‹: {checkpoint_path}")
    print(f"   è®­ç»ƒepoch: {checkpoint.get('epoch', 'N/A')}")
    val_loss = checkpoint.get('val_loss', None)
    if val_loss is not None:
        print(f"   éªŒè¯loss: {val_loss:.6f}")
    else:
        val_mse = checkpoint.get('val_mse', None)
        if val_mse is not None:
            print(f"   éªŒè¯MSE: {val_mse:.6f}")
        else:
            print(f"   éªŒè¯loss: N/A")
    return model

def get_image_transform():
    """è·å–å›¾åƒé¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    return transforms.Compose([
        transforms.Resize((240, 320)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

def check_success(ee_pos, marker_pos):
    """æ£€æŸ¥æ˜¯å¦æ»¡è¶³æˆåŠŸæ¡ä»¶"""
    diff_x = abs(ee_pos[0] - marker_pos[0])
    diff_y = abs(ee_pos[1] - marker_pos[1])
    diff_z = abs(ee_pos[2] - marker_pos[2])
    
    return (diff_x < SUCCESS_DISTANCE_X_MAX) and \
           (diff_y < SUCCESS_DISTANCE_Y_MAX) and \
           (diff_z < SUCCESS_DISTANCE_Z_MAX)

# ===================== ä¸»å‡½æ•° =====================

def main():
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆevaluate_bc.py åœ¨ training/ ç›®å½•ä¸‹ï¼‰
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)  # é¡¹ç›®æ ¹ç›®å½•
    default_output_dir = os.path.join(project_root, "evaluation")
    
    parser = argparse.ArgumentParser(description="BCæ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--num_episodes", type=int, default=20, help="è¯„ä¼°episodeæ•°é‡")
    parser.add_argument("--steps_per_episode", type=int, default=200, help="æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°")
    parser.add_argument("--save_images", action="store_true", help="æ˜¯å¦ä¿å­˜è¯„ä¼°è¿‡ç¨‹ä¸­çš„å›¾åƒ")
    parser.add_argument("--output_dir", type=str, default=default_output_dir, help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: <project_root>/evaluationï¼‰")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, device)
    transform = get_image_transform()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    if args.save_images:
        image_dir = os.path.join(args.output_dir, "images")
        os.makedirs(image_dir, exist_ok=True)

    # --- 1. åŠ è½½ç¯å¢ƒ ---
    print(f"\næ­£åœ¨åŠ è½½åœºæ™¯: {ENV_USD_PATH}")
    omni.usd.get_context().open_stage(ENV_USD_PATH)
    for _ in range(100):
        simulation_app.update()

    # --- 2. åˆå§‹åŒ–ä»¿çœŸ ---
    timeline = omni.timeline.get_timeline_interface()
    
    stage = omni.usd.get_context().get_stage()
    has_physics = False
    for prim in stage.Traverse():
        if prim.IsA(UsdPhysics.Scene):
            has_physics = True
            break
    if not has_physics:
        print("âš ï¸ åˆ›å»ºé»˜è®¤ PhysicsScene...")
        UsdPhysics.Scene.Define(stage, "/World/PhysicsScene")

    # ç¨³å®šæ¡Œå­
    table_prim = stage.GetPrimAtPath(TABLE_PATH)
    if table_prim.IsValid():
        if not table_prim.HasAPI(UsdPhysics.RigidBodyAPI):
            UsdPhysics.RigidBodyAPI.Apply(table_prim)
        UsdPhysics.RigidBodyAPI(table_prim).CreateKinematicEnabledAttr(True)

    # åˆ›å»ºæœºå™¨äººå¯¹è±¡
    print("åˆ›å»ºæœºå™¨äººå¯¹è±¡...")
    robot = Articulation(ROBOT_PATH)

    # åˆå§‹åŒ–ä»¿çœŸ
    print("åˆå§‹åŒ– SimulationContext...")
    sim = SimulationContext(physics_dt=DT, rendering_dt=DT, stage_units_in_meters=1.0)
    
    print("å¯åŠ¨ Timeline...")
    timeline.play()
    
    print("å¼ºåˆ¶åˆå§‹åŒ–ç‰©ç†å¼•æ“...")
    sim.initialize_physics()
    
    if not sim.is_playing():
        sim.play()

    # é¢„çƒ­
    print("æ­£åœ¨é¢„çƒ­ç‰©ç†å¼•æ“ (60å¸§)...")
    for _ in range(60):
        sim.step(render=False)

    # åˆå§‹åŒ–æœºå™¨äºº
    print("åˆå§‹åŒ–æœºå™¨äºº...")
    try:
        robot.initialize()
    except Exception as e:
        print(f"âš ï¸ ç¬¬ä¸€æ¬¡åˆå§‹åŒ–å¤±è´¥ ({e})ï¼Œå°è¯•é‡è¯•...")
        for _ in range(10):
            sim.step(render=False)
        robot.initialize()

    # åˆå§‹åŒ–ç›¸æœº
    cam = ViewportCamera(CAM_PATH)

    # è·å–markerä½ç½®
    marker_prim = XFormPrim(MARKER_PATH)
    marker_pos, marker_orn = marker_prim.get_world_pose()
    marker_pos = np.array([float(marker_pos[0]), float(marker_pos[1]), float(marker_pos[2])])

    # --- 3. è¯„ä¼°å¾ªç¯ ---
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼°: {args.num_episodes} ä¸ªepisodeï¼Œæ¯episodeæœ€å¤š {args.steps_per_episode} æ­¥")
    
    results = {
        "success": 0,
        "collision": 0,
        "timeout": 0,
        "episode_details": []
    }

    for episode_idx in range(args.num_episodes):
        print(f"\n{'='*60}")
        print(f"Episode {episode_idx + 1}/{args.num_episodes}")
        print(f"{'='*60}")

        # ä¸ºæ¯ä¸ª episode åˆ›å»ºç‹¬ç«‹çš„å­æ–‡ä»¶å¤¹
        episode_output_dir = os.path.join(args.output_dir, f"episode_{episode_idx:02d}")
        os.makedirs(episode_output_dir, exist_ok=True)

        # ä½¿ç”¨æ‹’ç»é‡‡æ ·æ‰¾åˆ°å·¥ä½œç©ºé—´å†…çš„æœ‰æ•ˆåˆå§‹é…ç½®ï¼ˆä¸æ•°æ®æ”¶é›†æ—¶ä¸€è‡´ï¼‰
        random_joint_positions, ee_pos_base = sample_valid_initial_config(robot, sim, max_attempts=100)
        
        if random_joint_positions is None:
            print(f"   âš ï¸ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆé…ç½®ï¼Œä½¿ç”¨éšæœºé…ç½®ï¼ˆå¯èƒ½ä¸åœ¨å·¥ä½œç©ºé—´å†…ï¼‰")
            random_joint_positions = sample_random_joint_config(robot.num_dof)
            robot.set_joint_velocities(np.zeros(robot.num_dof))
            robot.set_joint_positions(random_joint_positions)
        else:
            # é…ç½®å·²è®¾ç½®ï¼Œåªéœ€ç¡®ä¿ä½ç½®æ­£ç¡®
            robot.set_joint_velocities(np.zeros(robot.num_dof))
            robot.set_joint_positions(random_joint_positions)
        
        # ç‰©ç†ç¨³å®š
        for _ in range(30):
            sim.step(render=True)

        episode_success = False
        has_collision = False
        end_reason = "timeout"
        prev_dq = None

        # Episodeå¾ªç¯
        for step in range(args.steps_per_episode):
            if not simulation_app.is_running():
                break

            # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10æ­¥æ˜¾ç¤ºä¸€æ¬¡ï¼Œæˆ–æœ€åä¸€æ­¥ï¼‰
            if step % 10 == 0 or step == args.steps_per_episode - 1:
                progress_pct = (step + 1) / args.steps_per_episode * 100
                print(f"   æ­¥æ•°: {step + 1}/{args.steps_per_episode} ({progress_pct:.1f}%)", end="\r", flush=True)

            # 1. æ•è·å›¾åƒï¼ˆä¿å­˜åˆ°è¯¥ episode çš„å­æ–‡ä»¶å¤¹ä¸­ï¼‰
            temp_img_path = os.path.join(episode_output_dir, f"frame_{step:04d}.png")
            image_tensor = None
            max_capture_retries = 2  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œå› ä¸ºæ–‡ä»¶åå·²ä¸åŒ
            
            for capture_retry in range(max_capture_retries):
                # å¼ºåˆ¶æ¸²æŸ“æ›´æ–°ï¼ˆç¡®ä¿ viewport å·²æ¸²æŸ“ï¼‰
                simulation_app.update()
                
                # æ•è·å›¾åƒ
                if not cam.capture(temp_img_path):
                    if capture_retry < max_capture_retries - 1:
                        simulation_app.update()
                        time.sleep(0.05)
                        continue
                    else:
                        print(f"   âš ï¸ ç¬¬ {step} æ­¥æˆªå›¾å¤±è´¥ï¼ˆå·²é‡è¯• {max_capture_retries} æ¬¡ï¼‰")
                        break

                # å¼ºåˆ¶åˆ·æ–°ï¼ˆç¡®ä¿æ–‡ä»¶å†™å…¥å¼€å§‹ï¼‰
                simulation_app.update()
                
                # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆï¼ˆç®€åŒ–é€»è¾‘ï¼šåªè¦æ–‡ä»¶å¤§å° > æœ€å°é˜ˆå€¼å³å¯ï¼‰
                min_bytes = 10_000  # æœ€å°æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼ˆ1280x720 PNG ä¸€èˆ¬è¿œå¤§äºè¿™ä¸ªï¼‰
                max_wait_attempts = 20
                wait_attempt = 0
                file_ready = False
                
                while wait_attempt < max_wait_attempts:
                    if os.path.exists(temp_img_path):
                        file_size = os.path.getsize(temp_img_path)
                        if file_size >= min_bytes:
                            file_ready = True
                            break
                    simulation_app.update()  # æ¯æ¬¡æ£€æŸ¥æ—¶ä¹Ÿæ›´æ–°
                    time.sleep(0.05)
                    wait_attempt += 1
                
                if not file_ready:
                    if capture_retry < max_capture_retries - 1:
                        simulation_app.update()
                        time.sleep(0.05)
                        continue
                    else:
                        print(f"   âš ï¸ ç¬¬ {step} æ­¥å›¾åƒæ–‡ä»¶æœªå°±ç»ªï¼ˆå·²é‡è¯• {max_capture_retries} æ¬¡ï¼‰")
                        break

                # 2. é¢„å¤„ç†å›¾åƒ
                try:
                    # å°è¯•æ‰“å¼€å›¾åƒ
                    image = Image.open(temp_img_path).convert('RGB')
                    # éªŒè¯å›¾åƒå®Œæ•´æ€§
                    image.verify()  # éªŒè¯ä½†ä¸åŠ è½½æ•°æ®
                    image = Image.open(temp_img_path).convert('RGB')  # é‡æ–°æ‰“å¼€ä»¥åŠ è½½æ•°æ®
                    image_tensor = transform(image).unsqueeze(0).to(device)  # [1, 3, H, W]
                    break  # æˆåŠŸåŠ è½½ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                except Exception as e:
                    if capture_retry < max_capture_retries - 1:
                        simulation_app.update()
                        time.sleep(0.05)
                        continue
                    else:
                        print(f"   âš ï¸ å›¾åƒåŠ è½½å¤±è´¥: {e}ï¼ˆå·²é‡è¯• {max_capture_retries} æ¬¡ï¼‰")
                        break
            
            if image_tensor is None:
                continue  # è·³è¿‡è¿™ä¸€æ­¥ï¼Œç»§ç»­ä¸‹ä¸€æ­¥

            # 3. æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                delta_q_pred = model(image_tensor).cpu().numpy()[0]  # [7]

            # 4. åº”ç”¨åŠ¨ä½œï¼ˆdelta_q -> ç›®æ ‡å…³èŠ‚ä½ç½®ï¼‰
            # å°† delta_q è½¬æ¢ä¸ºç›®æ ‡å…³èŠ‚ä½ç½®ï¼ˆæ›´ç¬¦åˆ BC è®­ç»ƒçš„è¯­ä¹‰ï¼‰
            q_current = robot.get_joint_positions()
            
            # è®¡ç®—ç›®æ ‡å…³èŠ‚ä½ç½®ï¼šq_target = q_current + delta_q
            q_target = q_current + delta_q_pred
            
            # é™åˆ¶åœ¨å…³èŠ‚é™ä½å†…ï¼ˆé¿å…è¶…å‡ºç‰©ç†é™åˆ¶ï¼‰
            for i in range(len(q_target)):
                if i < len(PANDA_JOINT_LIMITS):
                    lower, upper = PANDA_JOINT_LIMITS[i]
                    q_target[i] = np.clip(q_target[i], lower, upper)
            
            # ä½¿ç”¨ apply_action åº”ç”¨ç›®æ ‡ä½ç½®ï¼ˆä¸æ•°æ®æ”¶é›†æ—¶ä¸€è‡´ï¼Œæ›´å¯é ï¼‰
            action = ArticulationAction(joint_positions=q_target)
            robot.apply_action(action)

            # 5. æ¨è¿›ä»¿çœŸ
            sim.step(render=True)
            simulation_app.update()  # ç¡®ä¿æ¸²æŸ“æ›´æ–°

            # 6. æ£€æŸ¥ç¢°æ’
            dq_after_step = robot.get_joint_velocities()
            max_velocity = np.max(np.abs(dq_after_step))
            
            if max_velocity > COLLISION_VELOCITY_THRESHOLD:
                has_collision = True
                end_reason = "collision"
                print(f"\n   âš ï¸ ç¬¬ {step + 1} æ­¥æ£€æµ‹åˆ°ç¢°æ’ï¼ˆé€Ÿåº¦å¼‚å¸¸: {max_velocity:.2f} rad/sï¼‰")
                break

            if prev_dq is not None:
                acceleration = (dq_after_step - prev_dq) / DT
                max_acceleration = np.max(np.abs(acceleration))
                if max_acceleration > COLLISION_ACCELERATION_THRESHOLD:
                    has_collision = True
                    end_reason = "collision"
                    print(f"\n   âš ï¸ ç¬¬ {step + 1} æ­¥æ£€æµ‹åˆ°ç¢°æ’ï¼ˆåŠ é€Ÿåº¦å¼‚å¸¸: {max_acceleration:.2f} rad/sÂ²ï¼‰")
                    break

            prev_dq = dq_after_step.copy()

            # 7. æ£€æŸ¥æˆåŠŸæ¡ä»¶
            try:
                tcp_prim = XFormPrim(TCP_PATH)
                tcp_pos, _ = tcp_prim.get_world_pose()
                tcp_pos = np.array([float(tcp_pos[0]), float(tcp_pos[1]), float(tcp_pos[2])])
                
                if check_success(tcp_pos, marker_pos):
                    episode_success = True
                    end_reason = "success"
                    diff_x = abs(tcp_pos[0] - marker_pos[0])
                    diff_y = abs(tcp_pos[1] - marker_pos[1])
                    diff_z = abs(tcp_pos[2] - marker_pos[2])
                    print(f"\n   âœ… ç¬¬ {step + 1} æ­¥æˆåŠŸåˆ°è¾¾ç›®æ ‡ï¼(X={diff_x:.3f}m, Y={diff_y:.3f}m, Z={diff_z:.3f}m)")
                    break
            except Exception as e:
                print(f"   âš ï¸ æ— æ³•è·å–TCPä½ç½®: {e}")

        # è®°å½•ç»“æœ
        if episode_success:
            results["success"] += 1
        elif has_collision:
            results["collision"] += 1
        else:
            results["timeout"] += 1

        episode_result = {
            "episode": episode_idx,
            "success": episode_success,
            "end_reason": end_reason,
            "end_step": step
        }
        results["episode_details"].append(episode_result)

        # ä¿å­˜è¯¥ episode çš„å•ç‹¬ç»“æœæ–‡ä»¶
        episode_result_file = os.path.join(episode_output_dir, "episode_result.json")
        with open(episode_result_file, "w") as f:
            json.dump(episode_result, f, indent=2)

        # æ¸…é™¤è¿›åº¦æ˜¾ç¤ºè¡Œ
        print(" " * 50, end="\r")  # æ¸…é™¤è¿›åº¦è¡Œ
        
        status_emoji = "âœ…" if episode_success else "âŒ"
        print(f"{status_emoji} Episode {episode_idx} å®Œæˆ: {end_reason} (å…± {step + 1} æ­¥)")

    # --- 4. æ‰“å°ç»Ÿè®¡ç»“æœ ---
    print(f"\n{'='*60}")
    print("è¯„ä¼°ç»“æœç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ€»episodeæ•°: {args.num_episodes}")
    print(f"æˆåŠŸ: {results['success']} ({results['success']/args.num_episodes*100:.1f}%)")
    print(f"ç¢°æ’: {results['collision']} ({results['collision']/args.num_episodes*100:.1f}%)")
    print(f"è¶…æ—¶: {results['timeout']} ({results['timeout']/args.num_episodes*100:.1f}%)")
    print(f"{'='*60}")

    # ä¿å­˜æ€»ä½“ç»“æœ
    results_file = os.path.join(args.output_dir, "evaluation_results.json")
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    print("\nè¯„ä¼°å®Œæˆï¼")
    simulation_app.close()

if __name__ == "__main__":
    main()

