#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Behavior Cloning æ¨¡å‹è¯„ä¼°è„šæœ¬
åœ¨Isaac Simä¸­åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¯„ä¼°æ€§èƒ½

è¿è¡Œæ–¹å¼ï¼ˆå¿…é¡»ä½¿ç”¨Isaac Simçš„Pythonï¼‰:
    ~/isaacsim/python.sh /home/alphatok/ME5400/training/evaluate_bc.py --checkpoint <path> --num_episodes 20

æˆ–è€…ä½¿ç”¨æä¾›çš„å¯åŠ¨è„šæœ¬:
    bash run_evaluate.sh <checkpoint_path> [num_episodes]
"""

from isaacsim import SimulationApp

# å¯åŠ¨ Isaac Sim
simulation_app = SimulationApp({"headless": False})

import os
import sys
import time
import argparse
import numpy as np
import torch
import torch.nn as nn
import torchvision.models as models
from torchvision import transforms
from PIL import Image

import omni.timeline
import omni.usd
from pxr import UsdPhysics, Gf, Usd

# æ ¸å¿ƒæ¨¡å—
from omni.isaac.core import SimulationContext
from isaacsim.core.prims import SingleArticulation as Articulation
from isaacsim.core.prims import SingleXFormPrim as XFormPrim
from omni.kit.viewport.utility import get_active_viewport, capture_viewport_to_file

# ===================== æ¨¡å‹å®šä¹‰ =====================

class ResNetMLPPolicy(nn.Module):
    """ResNet18 è§†è§‰ç¼–ç  + MLP åŠ¨ä½œå¤´ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    def __init__(self, out_dim=7):
        super().__init__()
        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # [B,512,1,1]
        self.head = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, out_dim),
        )

    def forward(self, x):
        feat = self.backbone(x).flatten(1)  # [B,512]
        return self.head(feat)              # [B,7]

# ===================== é…ç½® =====================

ENV_USD_PATH = "/home/alphatok/ME5400/env.setup/env.usda"
MARKER_PATH = "/World/Phantom/marker"
ROBOT_PATH = "/World/Panda"
TABLE_PATH = "/World/Table"
CAM_PATH = "/World/Panda/D405_rigid/D405/Camera_OmniVision_OV9782_Color"
TCP_PATH = "/World/Panda/TCP"

DT = 1.0 / 60.0

# Panda å…³èŠ‚é™åˆ¶
PANDA_JOINT_LIMITS = [
    (-2.8973, 2.8973),   # joint1
    (-1.7628, 1.7628),   # joint2
    (-2.8973, 2.8973),   # joint3
    (-3.0718, -0.0698),  # joint4
    (-2.8973, 2.8973),   # joint5
    (-0.0175, 3.7525),   # joint6
    (-2.8973, 2.8973),   # joint7
]

# æˆåŠŸæ¡ä»¶ï¼ˆä¸æ•°æ®æ”¶é›†æ—¶ä¸€è‡´ï¼‰
SUCCESS_DISTANCE_X_MAX = 0.1   # ç±³
SUCCESS_DISTANCE_Y_MAX = 0.1   # ç±³
SUCCESS_DISTANCE_Z_MAX = 0.3   # ç±³

# ç¢°æ’æ£€æµ‹é˜ˆå€¼
COLLISION_VELOCITY_THRESHOLD = 10.0  # rad/s
COLLISION_ACCELERATION_THRESHOLD = 50.0  # rad/sÂ²

# ===================== è¾…åŠ©å‡½æ•° =====================

class ViewportCamera:
    """è§†å£ç›¸æœºå°è£…"""
    def __init__(self, camera_path, resolution=(1280, 720)):
        self.viewport_api = get_active_viewport()
        if not self.viewport_api:
            raise RuntimeError("âŒ æ— æ³•æ‰¾åˆ°æ´»è·ƒè§†å£ï¼")
        self.viewport_api.camera_path = camera_path
        self.viewport_api.set_texture_resolution(resolution)

    def capture(self, filename):
        try:
            capture_viewport_to_file(self.viewport_api, filename)
            return True
        except Exception as e:
            print(f"âŒ æˆªå›¾å¼‚å¸¸: {e}")
            return False

def sample_random_joint_config(num_joints):
    """éšæœºé‡‡æ ·å…³èŠ‚é…ç½®"""
    random_joint_positions = []
    for i in range(num_joints):
        if i < len(PANDA_JOINT_LIMITS):
            lower, upper = PANDA_JOINT_LIMITS[i]
            random_joint_positions.append(np.random.uniform(lower, upper))
        else:
            random_joint_positions.append(np.random.uniform(-np.pi, np.pi))
    return np.array(random_joint_positions, dtype=np.float32)

def load_model(checkpoint_path, device):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    model = ResNetMLPPolicy(out_dim=7).to(device)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    print(f"âœ… å·²åŠ è½½æ¨¡å‹: {checkpoint_path}")
    print(f"   è®­ç»ƒepoch: {checkpoint.get('epoch', 'N/A')}")
    val_loss = checkpoint.get('val_loss', None)
    if val_loss is not None:
        print(f"   éªŒè¯loss: {val_loss:.6f}")
    else:
        val_mse = checkpoint.get('val_mse', None)
        if val_mse is not None:
            print(f"   éªŒè¯MSE: {val_mse:.6f}")
        else:
            print(f"   éªŒè¯loss: N/A")
    return model

def get_image_transform():
    """è·å–å›¾åƒé¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰"""
    return transforms.Compose([
        transforms.Resize((240, 320)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

def check_success(ee_pos, marker_pos):
    """æ£€æŸ¥æ˜¯å¦æ»¡è¶³æˆåŠŸæ¡ä»¶"""
    diff_x = abs(ee_pos[0] - marker_pos[0])
    diff_y = abs(ee_pos[1] - marker_pos[1])
    diff_z = abs(ee_pos[2] - marker_pos[2])
    
    return (diff_x < SUCCESS_DISTANCE_X_MAX) and \
           (diff_y < SUCCESS_DISTANCE_Y_MAX) and \
           (diff_z < SUCCESS_DISTANCE_Z_MAX)

# ===================== ä¸»å‡½æ•° =====================

def main():
    parser = argparse.ArgumentParser(description="BCæ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹checkpointè·¯å¾„")
    parser.add_argument("--num_episodes", type=int, default=20, help="è¯„ä¼°episodeæ•°é‡")
    parser.add_argument("--steps_per_episode", type=int, default=200, help="æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°")
    parser.add_argument("--save_images", action="store_true", help="æ˜¯å¦ä¿å­˜è¯„ä¼°è¿‡ç¨‹ä¸­çš„å›¾åƒ")
    parser.add_argument("--output_dir", type=str, default="./evaluation_results", help="è¾“å‡ºç›®å½•")
    args = parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹
    model = load_model(args.checkpoint, device)
    transform = get_image_transform()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    if args.save_images:
        image_dir = os.path.join(args.output_dir, "images")
        os.makedirs(image_dir, exist_ok=True)

    # --- 1. åŠ è½½ç¯å¢ƒ ---
    print(f"\næ­£åœ¨åŠ è½½åœºæ™¯: {ENV_USD_PATH}")
    omni.usd.get_context().open_stage(ENV_USD_PATH)
    for _ in range(100):
        simulation_app.update()

    # --- 2. åˆå§‹åŒ–ä»¿çœŸ ---
    timeline = omni.timeline.get_timeline_interface()
    
    stage = omni.usd.get_context().get_stage()
    has_physics = False
    for prim in stage.Traverse():
        if prim.IsA(UsdPhysics.Scene):
            has_physics = True
            break
    if not has_physics:
        print("âš ï¸ åˆ›å»ºé»˜è®¤ PhysicsScene...")
        UsdPhysics.Scene.Define(stage, "/World/PhysicsScene")

    # ç¨³å®šæ¡Œå­
    table_prim = stage.GetPrimAtPath(TABLE_PATH)
    if table_prim.IsValid():
        if not table_prim.HasAPI(UsdPhysics.RigidBodyAPI):
            UsdPhysics.RigidBodyAPI.Apply(table_prim)
        UsdPhysics.RigidBodyAPI(table_prim).CreateKinematicEnabledAttr(True)

    # åˆ›å»ºæœºå™¨äººå¯¹è±¡
    print("åˆ›å»ºæœºå™¨äººå¯¹è±¡...")
    robot = Articulation(ROBOT_PATH)

    # åˆå§‹åŒ–ä»¿çœŸ
    print("åˆå§‹åŒ– SimulationContext...")
    sim = SimulationContext(physics_dt=DT, rendering_dt=DT, stage_units_in_meters=1.0)
    
    print("å¯åŠ¨ Timeline...")
    timeline.play()
    
    print("å¼ºåˆ¶åˆå§‹åŒ–ç‰©ç†å¼•æ“...")
    sim.initialize_physics()
    
    if not sim.is_playing():
        sim.play()

    # é¢„çƒ­
    print("æ­£åœ¨é¢„çƒ­ç‰©ç†å¼•æ“ (60å¸§)...")
    for _ in range(60):
        sim.step(render=False)

    # åˆå§‹åŒ–æœºå™¨äºº
    print("åˆå§‹åŒ–æœºå™¨äºº...")
    try:
        robot.initialize()
    except Exception as e:
        print(f"âš ï¸ ç¬¬ä¸€æ¬¡åˆå§‹åŒ–å¤±è´¥ ({e})ï¼Œå°è¯•é‡è¯•...")
        for _ in range(10):
            sim.step(render=False)
        robot.initialize()

    # åˆå§‹åŒ–ç›¸æœº
    cam = ViewportCamera(CAM_PATH)

    # è·å–markerä½ç½®
    marker_prim = XFormPrim(MARKER_PATH)
    marker_pos, marker_orn = marker_prim.get_world_pose()
    marker_pos = np.array([float(marker_pos[0]), float(marker_pos[1]), float(marker_pos[2])])

    # --- 3. è¯„ä¼°å¾ªç¯ ---
    print(f"\nğŸš€ å¼€å§‹è¯„ä¼°: {args.num_episodes} ä¸ªepisodeï¼Œæ¯episodeæœ€å¤š {args.steps_per_episode} æ­¥")
    
    results = {
        "success": 0,
        "collision": 0,
        "timeout": 0,
        "episode_details": []
    }

    for episode_idx in range(args.num_episodes):
        print(f"\n{'='*60}")
        print(f"Episode {episode_idx + 1}/{args.num_episodes}")
        print(f"{'='*60}")

        # éšæœºåˆå§‹é…ç½®
        random_joint_positions = sample_random_joint_config(robot.num_dof)
        robot.set_joint_velocities(np.zeros(robot.num_dof))
        robot.set_joint_positions(random_joint_positions)
        
        # ç‰©ç†ç¨³å®š
        for _ in range(30):
            sim.step(render=True)

        episode_success = False
        has_collision = False
        end_reason = "timeout"
        prev_dq = None

        # Episodeå¾ªç¯
        for step in range(args.steps_per_episode):
            if not simulation_app.is_running():
                break

            # 1. æ•è·å›¾åƒ
            temp_img_path = os.path.join(args.output_dir, "temp_frame.png")
            if not cam.capture(temp_img_path):
                print(f"   âš ï¸ ç¬¬ {step} æ­¥æˆªå›¾å¤±è´¥")
                continue

            # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆï¼ˆcapture_viewport_to_fileå¯èƒ½æ˜¯å¼‚æ­¥çš„ï¼‰
            max_wait_attempts = 10
            wait_attempt = 0
            while wait_attempt < max_wait_attempts:
                if os.path.exists(temp_img_path):
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œç¡®ä¿ä¸æ˜¯ç©ºæ–‡ä»¶
                    file_size = os.path.getsize(temp_img_path)
                    if file_size > 0:
                        # å†ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿æ–‡ä»¶å®Œå…¨å†™å…¥
                        time.sleep(0.05)
                        break
                time.sleep(0.05)
                wait_attempt += 1
            
            if wait_attempt >= max_wait_attempts or not os.path.exists(temp_img_path):
                print(f"   âš ï¸ ç¬¬ {step} æ­¥å›¾åƒæ–‡ä»¶æœªç”Ÿæˆæˆ–ä¸ºç©º")
                continue

            # 2. é¢„å¤„ç†å›¾åƒ
            try:
                # å°è¯•æ‰“å¼€å›¾åƒï¼Œå¦‚æœå¤±è´¥ä¼šæŠ›å‡ºå¼‚å¸¸
                image = Image.open(temp_img_path).convert('RGB')
                # å°è¯•åŠ è½½å›¾åƒæ•°æ®ï¼Œç¡®ä¿æ–‡ä»¶å®Œæ•´
                image.load()  # è¿™ä¼šå¼ºåˆ¶åŠ è½½æ‰€æœ‰æ•°æ®ï¼Œå¦‚æœæ–‡ä»¶æŸåä¼šæŠ›å‡ºå¼‚å¸¸
                image_tensor = transform(image).unsqueeze(0).to(device)  # [1, 3, H, W]
            except Exception as e:
                print(f"   âš ï¸ å›¾åƒåŠ è½½å¤±è´¥: {e}")
                continue

            # 3. æ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                delta_q_pred = model(image_tensor).cpu().numpy()[0]  # [7]

            # 4. åº”ç”¨åŠ¨ä½œï¼ˆdelta_q -> å…³èŠ‚é€Ÿåº¦æ§åˆ¶ï¼‰
            # ä½¿ç”¨ç®€å•çš„é€Ÿåº¦æ§åˆ¶ï¼šå°†delta_qè½¬æ¢ä¸ºé€Ÿåº¦å‘½ä»¤
            q_current = robot.get_joint_positions()
            
            # å°†delta_qè½¬æ¢ä¸ºé€Ÿåº¦ï¼ˆå¸¦ç¼©æ”¾å› å­ï¼Œé¿å…è¿‡å¤§é€Ÿåº¦ï¼‰
            max_velocity = 2.0  # rad/sï¼Œæœ€å¤§å…³èŠ‚é€Ÿåº¦
            velocity_scale = 10.0  # å°†delta_qè½¬æ¢ä¸ºé€Ÿåº¦çš„ç¼©æ”¾å› å­
            target_velocity = np.clip(delta_q_pred * velocity_scale, -max_velocity, max_velocity)
            
            # åº”ç”¨é€Ÿåº¦å‘½ä»¤
            robot.set_joint_velocities(target_velocity)

            # 5. æ¨è¿›ä»¿çœŸ
            sim.step(render=True)

            # 6. æ£€æŸ¥ç¢°æ’
            dq_after_step = robot.get_joint_velocities()
            max_velocity = np.max(np.abs(dq_after_step))
            
            if max_velocity > COLLISION_VELOCITY_THRESHOLD:
                has_collision = True
                end_reason = "collision"
                print(f"   âš ï¸ ç¬¬ {step} æ­¥æ£€æµ‹åˆ°ç¢°æ’ï¼ˆé€Ÿåº¦å¼‚å¸¸: {max_velocity:.2f} rad/sï¼‰")
                break

            if prev_dq is not None:
                acceleration = (dq_after_step - prev_dq) / DT
                max_acceleration = np.max(np.abs(acceleration))
                if max_acceleration > COLLISION_ACCELERATION_THRESHOLD:
                    has_collision = True
                    end_reason = "collision"
                    print(f"   âš ï¸ ç¬¬ {step} æ­¥æ£€æµ‹åˆ°ç¢°æ’ï¼ˆåŠ é€Ÿåº¦å¼‚å¸¸: {max_acceleration:.2f} rad/sÂ²ï¼‰")
                    break

            prev_dq = dq_after_step.copy()

            # 7. æ£€æŸ¥æˆåŠŸæ¡ä»¶
            try:
                tcp_prim = XFormPrim(TCP_PATH)
                tcp_pos, _ = tcp_prim.get_world_pose()
                tcp_pos = np.array([float(tcp_pos[0]), float(tcp_pos[1]), float(tcp_pos[2])])
                
                if check_success(tcp_pos, marker_pos):
                    episode_success = True
                    end_reason = "success"
                    diff_x = abs(tcp_pos[0] - marker_pos[0])
                    diff_y = abs(tcp_pos[1] - marker_pos[1])
                    diff_z = abs(tcp_pos[2] - marker_pos[2])
                    print(f"   âœ… ç¬¬ {step} æ­¥æˆåŠŸåˆ°è¾¾ç›®æ ‡ï¼(X={diff_x:.3f}m, Y={diff_y:.3f}m, Z={diff_z:.3f}m)")
                    break
            except Exception as e:
                print(f"   âš ï¸ æ— æ³•è·å–TCPä½ç½®: {e}")

        # è®°å½•ç»“æœ
        if episode_success:
            results["success"] += 1
        elif has_collision:
            results["collision"] += 1
        else:
            results["timeout"] += 1

        results["episode_details"].append({
            "episode": episode_idx,
            "success": episode_success,
            "end_reason": end_reason,
            "end_step": step
        })

        status_emoji = "âœ…" if episode_success else "âŒ"
        print(f"{status_emoji} Episode {episode_idx} å®Œæˆ: {end_reason}")

    # --- 4. æ‰“å°ç»Ÿè®¡ç»“æœ ---
    print(f"\n{'='*60}")
    print("è¯„ä¼°ç»“æœç»Ÿè®¡")
    print(f"{'='*60}")
    print(f"æ€»episodeæ•°: {args.num_episodes}")
    print(f"æˆåŠŸ: {results['success']} ({results['success']/args.num_episodes*100:.1f}%)")
    print(f"ç¢°æ’: {results['collision']} ({results['collision']/args.num_episodes*100:.1f}%)")
    print(f"è¶…æ—¶: {results['timeout']} ({results['timeout']/args.num_episodes*100:.1f}%)")
    print(f"{'='*60}")

    # ä¿å­˜ç»“æœ
    import json
    results_file = os.path.join(args.output_dir, "evaluation_results.json")
    with open(results_file, "w") as f:
        json.dump(results, f, indent=2)
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    print("\nè¯„ä¼°å®Œæˆï¼")
    simulation_app.close()

if __name__ == "__main__":
    main()

