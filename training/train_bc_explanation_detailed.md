# train_bc.py é€è¡Œè¯¦ç»†è§£é‡Š

## æ–‡ä»¶æ•´ä½“åŠŸèƒ½

è¿™ä¸ªæ–‡ä»¶å®ç°äº†ä¸€ä¸ª**ç®¡ç†å¼è¡Œä¸ºå…‹éš†ï¼ˆManaged Behavior Cloningï¼‰è®­ç»ƒè„šæœ¬**ï¼Œä½¿ç”¨ ResNet18 ä½œä¸ºè§†è§‰ç¼–ç å™¨ï¼ŒMLP ä½œä¸ºåŠ¨ä½œé¢„æµ‹å¤´ã€‚å®ƒåŒ…å«å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ—©åœã€å­¦ä¹ ç‡è°ƒåº¦ã€æŒ‡æ ‡è®°å½•å’Œå¯è§†åŒ–åŠŸèƒ½ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¼å…¥åº“ï¼ˆç¬¬14-36è¡Œï¼‰

```python
import os
import csv
import json
import time
import math
import random
import argparse
from dataclasses import dataclass
from typing import Dict, Any, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.models as models
from tqdm import tqdm

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from dataset import MarkerDataset
```

**é€è¡Œè§£é‡Šï¼š**

- **`os`**: ç”¨äºæ–‡ä»¶è·¯å¾„æ“ä½œï¼ˆ`os.path.join`, `os.makedirs`ç­‰ï¼‰
- **`csv`**: ç”¨äºå†™å…¥ CSV æ ¼å¼çš„æŒ‡æ ‡æ–‡ä»¶
- **`json`**: ç”¨äºä¿å­˜é…ç½®å’Œ JSONL æ ¼å¼çš„æŒ‡æ ‡
- **`time`**: ç”¨äºè®°å½•è®­ç»ƒæ—¶é—´
- **`math`**: æ•°å­¦å‡½æ•°ï¼ˆè™½ç„¶ä»£ç ä¸­å¯èƒ½æœªç›´æ¥ä½¿ç”¨ï¼‰
- **`random`**: ç”¨äºè®¾ç½®éšæœºç§å­
- **`argparse`**: ç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
- **`dataclass`**: ç”¨äºå®šä¹‰æ•°æ®ç±»ï¼ˆ`EvalStats`ï¼‰
- **`typing`**: ç±»å‹æç¤ºï¼ˆ`Dict`, `Any`, `Tuple`ï¼‰

- **`numpy as np`**: æ•°å€¼è®¡ç®—åº“
- **`torch`**: PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
- **`torch.nn as nn`**: ç¥ç»ç½‘ç»œæ¨¡å—
- **`torch.optim as optim`**: ä¼˜åŒ–å™¨
- **`DataLoader`**: PyTorch æ•°æ®åŠ è½½å™¨
- **`torchvision.models`**: é¢„è®­ç»ƒæ¨¡å‹ï¼ˆResNet18ï¼‰
- **`tqdm`**: è¿›åº¦æ¡åº“

- **`matplotlib`**: ç»˜å›¾åº“
  - **`matplotlib.use("Agg")`**: è®¾ç½®åç«¯ä¸º "Agg"ï¼ˆæ—  GUIï¼Œé€‚åˆæœåŠ¡å™¨ç¯å¢ƒï¼‰
- **`matplotlib.pyplot as plt`**: ç»˜å›¾æ¥å£
- **`from dataset import MarkerDataset`**: å¯¼å…¥è‡ªå®šä¹‰æ•°æ®é›†ç±»

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šå·¥å…·å‡½æ•°ï¼ˆç¬¬39-86è¡Œï¼‰

### `set_seed(seed: int = 42)`ï¼ˆç¬¬41-45è¡Œï¼‰

```python
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
```

**åŠŸèƒ½**ï¼šè®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ï¼Œç¡®ä¿å®éªŒå¯å¤ç°ã€‚

**é€è¡Œè§£é‡Šï¼š**
- `random.seed(seed)`: è®¾ç½® Python å†…ç½®éšæœºæ•°ç”Ÿæˆå™¨
- `np.random.seed(seed)`: è®¾ç½® NumPy éšæœºæ•°ç”Ÿæˆå™¨
- `torch.manual_seed(seed)`: è®¾ç½® PyTorch CPU éšæœºæ•°ç”Ÿæˆå™¨
- `torch.cuda.manual_seed_all(seed)`: è®¾ç½®æ‰€æœ‰ CUDA è®¾å¤‡çš„éšæœºæ•°ç”Ÿæˆå™¨

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- **å¯å¤ç°æ€§**ï¼šç›¸åŒçš„ç§å­ä¼šäº§ç”Ÿç›¸åŒçš„éšæœºæ•°åºåˆ—
- **è°ƒè¯•**ï¼šä¾¿äºé‡ç° bug å’Œå¯¹æ¯”ä¸åŒå®éªŒ

---

### `ensure_dir(p: str)`ï¼ˆç¬¬47-48è¡Œï¼‰

```python
def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)
```

**åŠŸèƒ½**ï¼šç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºã€‚

**å‚æ•°**ï¼š
- `p`: ç›®å½•è·¯å¾„

**`exist_ok=True`**ï¼šå¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œä¸æŠ›å‡ºå¼‚å¸¸

---

### `to_float(x)`ï¼ˆç¬¬50-54è¡Œï¼‰

```python
def to_float(x):
    try:
        return float(x)
    except Exception:
        return float("nan")
```

**åŠŸèƒ½**ï¼šå®‰å…¨åœ°å°†å€¼è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¦‚æœè½¬æ¢å¤±è´¥è¿”å› NaNã€‚

**ä½¿ç”¨åœºæ™¯**ï¼šå¤„ç†å¯èƒ½æ— æ•ˆçš„æ•°å€¼ï¼ˆä¾‹å¦‚ä» CSV è¯»å–çš„æ•°æ®ï¼‰

---

### `safe_mean(xs)`ï¼ˆç¬¬56-58è¡Œï¼‰

```python
def safe_mean(xs):
    xs = [x for x in xs if np.isfinite(x)]
    return float(np.mean(xs)) if len(xs) else float("nan")
```

**åŠŸèƒ½**ï¼šè®¡ç®—åˆ—è¡¨çš„å¹³å‡å€¼ï¼Œè‡ªåŠ¨è¿‡æ»¤æ‰ NaN å’Œ Infã€‚

**é€è¡Œè§£é‡Šï¼š**
- `[x for x in xs if np.isfinite(x)]`: åˆ—è¡¨æ¨å¯¼å¼ï¼Œåªä¿ç•™æœ‰é™æ•°ï¼ˆä¸æ˜¯ NaN æˆ– Infï¼‰
- `np.mean(xs)`: è®¡ç®—å¹³å‡å€¼
- `if len(xs) else float("nan")`: å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œè¿”å› NaN

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- æŸäº›æŒ‡æ ‡å¯èƒ½åŒ…å«æ— æ•ˆå€¼ï¼Œéœ€è¦å®‰å…¨å¤„ç†

---

### `cosine_similarity(a, b, eps)`ï¼ˆç¬¬60-64è¡Œï¼‰

```python
def cosine_similarity(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    # a,b: [B, D]
    an = torch.norm(a, dim=1).clamp_min(eps)
    bn = torch.norm(b, dim=1).clamp_min(eps)
    return (a * b).sum(dim=1) / (an * bn)
```

**åŠŸèƒ½**ï¼šè®¡ç®—ä¸¤ä¸ªå¼ é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé€æ ·æœ¬ï¼‰ã€‚

**é€è¡Œè§£é‡Šï¼š**
- **è¾“å…¥**ï¼š`a` å’Œ `b` éƒ½æ˜¯å½¢çŠ¶ `[B, D]` çš„å¼ é‡ï¼ˆB æ˜¯ batch sizeï¼ŒD æ˜¯ç»´åº¦ï¼‰
- **`torch.norm(a, dim=1)`**: è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ L2 èŒƒæ•°ï¼Œç»“æœå½¢çŠ¶ `[B]`
- **`.clamp_min(eps)`**: å°†èŒƒæ•°é™åˆ¶åœ¨æœ€å°å€¼ `eps` ä»¥ä¸Šï¼Œé¿å…é™¤ä»¥é›¶
- **`(a * b).sum(dim=1)`**: é€å…ƒç´ ç›¸ä¹˜åæ±‚å’Œï¼Œå¾—åˆ°å†…ç§¯ï¼Œå½¢çŠ¶ `[B]`
- **`/ (an * bn)`**: é™¤ä»¥ä¸¤ä¸ªèŒƒæ•°çš„ä¹˜ç§¯ï¼Œå¾—åˆ°ä½™å¼¦ç›¸ä¼¼åº¦

**å…¬å¼**ï¼š`cos(Î¸) = (a Â· b) / (||a|| * ||b||)`

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- è¯„ä¼°é¢„æµ‹åŠ¨ä½œçš„æ–¹å‘æ˜¯å¦ä¸çœŸå®åŠ¨ä½œä¸€è‡´ï¼ˆä¸å…³å¿ƒå¹…åº¦ï¼‰

---

### `plot_curve(x, ys, title, xlabel, ylabel, out_path)`ï¼ˆç¬¬66-76è¡Œï¼‰

```python
def plot_curve(x, ys: Dict[str, list], title: str, xlabel: str, ylabel: str, out_path: str):
    plt.figure()
    for k, v in ys.items():
        plt.plot(x, v, label=k)
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
```

**åŠŸèƒ½**ï¼šç»˜åˆ¶ä¸€æ¡æˆ–å¤šæ¡æ›²çº¿ã€‚

**é€è¡Œè§£é‡Šï¼š**
- **`plt.figure()`**: åˆ›å»ºæ–°å›¾å½¢
- **`for k, v in ys.items()`**: éå†å­—å…¸ï¼Œ`k` æ˜¯æ›²çº¿åç§°ï¼Œ`v` æ˜¯ y å€¼åˆ—è¡¨
- **`plt.plot(x, v, label=k)`**: ç»˜åˆ¶æ›²çº¿ï¼Œ`x` æ˜¯ x è½´å€¼ï¼Œ`v` æ˜¯ y è½´å€¼
- **`plt.title(title)`**: è®¾ç½®æ ‡é¢˜
- **`plt.xlabel(xlabel)`**: è®¾ç½® x è½´æ ‡ç­¾
- **`plt.ylabel(ylabel)`**: è®¾ç½® y è½´æ ‡ç­¾
- **`plt.legend()`**: æ˜¾ç¤ºå›¾ä¾‹
- **`plt.tight_layout()`**: è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé¿å…æ ‡ç­¾é‡å 
- **`plt.savefig(out_path)`**: ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶
- **`plt.close()`**: å…³é—­å›¾å½¢ï¼Œé‡Šæ”¾å†…å­˜

**ä½¿ç”¨åœºæ™¯**ï¼šç»˜åˆ¶è®­ç»ƒ/éªŒè¯ loss æ›²çº¿ã€å­¦ä¹ ç‡æ›²çº¿ç­‰

---

### `plot_hist(data, title, xlabel, out_path, bins)`ï¼ˆç¬¬78-86è¡Œï¼‰

```python
def plot_hist(data: np.ndarray, title: str, xlabel: str, out_path: str, bins: int = 50):
    plt.figure()
    plt.hist(data, bins=bins)
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel("count")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
```

**åŠŸèƒ½**ï¼šç»˜åˆ¶ç›´æ–¹å›¾ã€‚

**å‚æ•°**ï¼š
- `data`: æ•°æ®æ•°ç»„
- `bins`: ç›´æ–¹å›¾çš„ bin æ•°é‡ï¼ˆé»˜è®¤ 50ï¼‰

**ä½¿ç”¨åœºæ™¯**ï¼šå¯è§†åŒ–æ•°æ®åˆ†å¸ƒï¼ˆä¾‹å¦‚åŠ¨ä½œèŒƒæ•°åˆ†å¸ƒï¼‰

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°æ®åŠ è½½ï¼ˆç¬¬89-111è¡Œï¼‰

### `make_loaders(dataset_root, batch_size, num_workers, image_size_hw, only_success)`ï¼ˆç¬¬91-111è¡Œï¼‰

```python
def make_loaders(dataset_root, batch_size=64, num_workers=4, image_size_hw=(240, 320), only_success=False):
    train_set = MarkerDataset(dataset_root, split="train", image_size_hw=image_size_hw, only_success=only_success)
    val_set = MarkerDataset(dataset_root, split="val", image_size_hw=image_size_hw, only_success=only_success)

    train_loader = DataLoader(
        train_set,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )
    val_loader = DataLoader(
        val_set,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False,
    )
    return train_set, val_set, train_loader, val_loader
```

**åŠŸèƒ½**ï¼šåˆ›å»ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œå¯¹åº”çš„ DataLoaderã€‚

**é€è¡Œè§£é‡Šï¼š**

**ç¬¬92-93è¡Œï¼šåˆ›å»ºæ•°æ®é›†**
- `MarkerDataset(..., split="train")`: åˆ›å»ºè®­ç»ƒé›†ï¼ˆ80% çš„æ•°æ®ï¼‰
- `MarkerDataset(..., split="val")`: åˆ›å»ºéªŒè¯é›†ï¼ˆ20% çš„æ•°æ®ï¼‰
- `only_success=only_success`: å¦‚æœä¸º Trueï¼ŒåªåŠ è½½æˆåŠŸçš„ episode

**ç¬¬95-102è¡Œï¼šè®­ç»ƒé›† DataLoader**
- `batch_size=batch_size`: æ¯æ‰¹æ ·æœ¬æ•°é‡
- `shuffle=True`: **æ‰“ä¹±æ•°æ®é¡ºåº**ï¼ˆè®­ç»ƒæ—¶éœ€è¦ï¼‰
- `num_workers=num_workers`: **å¹¶è¡ŒåŠ è½½æ•°æ®çš„è¿›ç¨‹æ•°**ï¼ˆåŠ é€Ÿæ•°æ®åŠ è½½ï¼‰
- `pin_memory=True`: **å›ºå®šå†…å­˜**ï¼ŒåŠ é€Ÿ GPU ä¼ è¾“
- `drop_last=True`: **ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„ batch**ï¼ˆä¿è¯ batch å¤§å°ä¸€è‡´ï¼‰

**ç¬¬103-110è¡Œï¼šéªŒè¯é›† DataLoader**
- `shuffle=False`: **ä¸æ‰“ä¹±æ•°æ®**ï¼ˆéªŒè¯æ—¶ä¸éœ€è¦ï¼‰
- `drop_last=False`: **ä¿ç•™æœ€åä¸€ä¸ªä¸å®Œæ•´çš„ batch**ï¼ˆä¸æµªè´¹æ•°æ®ï¼‰

**ä¸ºä»€ä¹ˆè®­ç»ƒé›† `drop_last=True`ï¼Ÿ**
- æŸäº›æ“ä½œï¼ˆå¦‚ BatchNormï¼‰éœ€è¦å›ºå®š batch size
- æœ€åä¸€ä¸ª batch å¯èƒ½å¾ˆå°ï¼Œå¯¼è‡´ç»Ÿè®¡ä¸ç¨³å®š

**ä¸ºä»€ä¹ˆéªŒè¯é›† `drop_last=False`ï¼Ÿ**
- éªŒè¯æ—¶ä¸éœ€è¦å›ºå®š batch size
- ä¿ç•™æ‰€æœ‰æ•°æ®ï¼Œè¯„ä¼°æ›´å‡†ç¡®

---

## ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹å®šä¹‰ï¼ˆç¬¬114-146è¡Œï¼‰

### `ResNetMLPPolicy` ç±»ï¼ˆç¬¬116-131è¡Œï¼‰

```python
class ResNetMLPPolicy(nn.Module):
    def __init__(self, out_dim=7):
        super().__init__()
        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
        self.backbone = nn.Sequential(*list(backbone.children())[:-1])  # [B,512,1,1]
        self.head = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, out_dim),
        )

    def forward(self, x):
        feat = self.backbone(x).flatten(1)  # [B,512]
        return self.head(feat)              # [B,7]
```

**åŠŸèƒ½**ï¼šå®šä¹‰ ResNet18 + MLP ç­–ç•¥ç½‘ç»œã€‚

**é€è¡Œè§£é‡Šï¼š**

**`__init__` æ–¹æ³•ï¼š**

**ç¬¬119è¡Œï¼šåŠ è½½é¢„è®­ç»ƒ ResNet18**
```python
backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
```
- åŠ è½½åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒçš„ ResNet18
- **é¢„è®­ç»ƒçš„å¥½å¤„**ï¼šå¯ä»¥åˆ©ç”¨åœ¨è‡ªç„¶å›¾åƒä¸Šå­¦åˆ°çš„ç‰¹å¾

**ç¬¬120è¡Œï¼šç§»é™¤æœ€åä¸€å±‚ï¼ˆåˆ†ç±»å¤´ï¼‰**
```python
self.backbone = nn.Sequential(*list(backbone.children())[:-1])
```
- `backbone.children()`: è·å– ResNet18 çš„æ‰€æœ‰å­æ¨¡å—
- `list(...)`: è½¬æ¢ä¸ºåˆ—è¡¨
- `[:-1]`: åˆ‡ç‰‡ï¼Œå»æ‰æœ€åä¸€ä¸ªå…ƒç´ ï¼ˆåˆ†ç±»å¤´ `fc`ï¼‰
- `nn.Sequential(*...)`: é‡æ–°ç»„åˆä¸º Sequential
- **è¾“å‡ºå½¢çŠ¶**ï¼š`[B, 512, 1, 1]`ï¼ˆB æ˜¯ batch sizeï¼Œ512 æ˜¯ç‰¹å¾ç»´åº¦ï¼‰

**ç¬¬121-127è¡Œï¼šå®šä¹‰ MLP åŠ¨ä½œå¤´**
```python
self.head = nn.Sequential(
    nn.Linear(512, 256),    # 512 -> 256
    nn.ReLU(inplace=True),  # ReLU æ¿€æ´»
    nn.Linear(256, 128),    # 256 -> 128
    nn.ReLU(inplace=True),  # ReLU æ¿€æ´»
    nn.Linear(128, out_dim), # 128 -> 7ï¼ˆ7ä¸ªå…³èŠ‚ï¼‰
)
```
- **`nn.Linear(512, 256)`**: å…¨è¿æ¥å±‚ï¼Œè¾“å…¥ 512 ç»´ï¼Œè¾“å‡º 256 ç»´
- **`nn.ReLU(inplace=True)`**: ReLU æ¿€æ´»å‡½æ•°ï¼Œ`inplace=True` è¡¨ç¤ºåŸåœ°æ“ä½œï¼ˆèŠ‚çœå†…å­˜ï¼‰
- **`out_dim=7`**: è¾“å‡º 7 ç»´ï¼ˆPanda æœºæ¢°è‡‚æœ‰ 7 ä¸ªå…³èŠ‚ï¼‰

**`forward` æ–¹æ³•ï¼š**

**ç¬¬130è¡Œï¼šæå–è§†è§‰ç‰¹å¾**
```python
feat = self.backbone(x).flatten(1)  # [B,512]
```
- `self.backbone(x)`: è¾“å…¥å›¾åƒ `[B, 3, H, W]`ï¼Œè¾“å‡ºç‰¹å¾ `[B, 512, 1, 1]`
- `.flatten(1)`: å°† `[B, 512, 1, 1]` å±•å¹³ä¸º `[B, 512]`ï¼ˆä¿ç•™ batch ç»´åº¦ï¼‰

**ç¬¬131è¡Œï¼šé¢„æµ‹åŠ¨ä½œ**
```python
return self.head(feat)  # [B,7]
```
- è¾“å…¥ `[B, 512]`ï¼Œè¾“å‡º `[B, 7]`ï¼ˆ7 ä¸ªå…³èŠ‚çš„å¢é‡å‘½ä»¤ï¼‰

---

### `freeze_backbone(model, freeze)`ï¼ˆç¬¬133-135è¡Œï¼‰

```python
def freeze_backbone(model: ResNetMLPPolicy, freeze: bool = True):
    for p in model.backbone.parameters():
        p.requires_grad = not freeze
```

**åŠŸèƒ½**ï¼šå†»ç»“æˆ–è§£å†» backbone çš„å‚æ•°ã€‚

**é€è¡Œè§£é‡Šï¼š**
- `model.backbone.parameters()`: è·å– backbone çš„æ‰€æœ‰å‚æ•°
- `p.requires_grad = not freeze`: 
  - å¦‚æœ `freeze=True`ï¼Œåˆ™ `requires_grad=False`ï¼ˆå‚æ•°ä¸æ›´æ–°ï¼‰
  - å¦‚æœ `freeze=False`ï¼Œåˆ™ `requires_grad=True`ï¼ˆå‚æ•°å¯æ›´æ–°ï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ**
- **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼šå…ˆå†»ç»“ backboneï¼Œåªè®­ç»ƒ headï¼ˆæ›´å¿«ã€æ›´ç¨³å®šï¼‰
- ç„¶åè§£å†» backboneï¼Œè¿›è¡Œç«¯åˆ°ç«¯å¾®è°ƒ

---

### `unfreeze_layer4_only(model)`ï¼ˆç¬¬137-146è¡Œï¼‰

```python
def unfreeze_layer4_only(model: ResNetMLPPolicy):
    # backbone is a Sequential of resnet children excluding fc:
    # [conv1,bn1,relu,maxpool,layer1,layer2,layer3,layer4,avgpool]
    # We want layer4 trainable, others frozen.
    for p in model.backbone.parameters():
        p.requires_grad = False
    # layer4 is index 7 in that sequence
    layer4 = model.backbone[7]
    for p in layer4.parameters():
        p.requires_grad = True
```

**åŠŸèƒ½**ï¼šåªè§£å†» ResNet18 çš„ layer4ï¼Œå…¶ä»–å±‚ä¿æŒå†»ç»“ã€‚

**é€è¡Œè§£é‡Šï¼š**

**ç¬¬141-142è¡Œï¼šå†»ç»“æ‰€æœ‰ backbone å‚æ•°**
```python
for p in model.backbone.parameters():
    p.requires_grad = False
```

**ç¬¬144è¡Œï¼šè·å– layer4**
```python
layer4 = model.backbone[7]
```
- ResNet18 çš„ backbone Sequential ç»“æ„ï¼š
  - `[0]`: conv1
  - `[1]`: bn1
  - `[2]`: relu
  - `[3]`: maxpool
  - `[4]`: layer1
  - `[5]`: layer2
  - `[6]`: layer3
  - `[7]`: layer4 â† æˆ‘ä»¬è¦è§£å†»çš„å±‚
  - `[8]`: avgpool

**ç¬¬145-146è¡Œï¼šè§£å†» layer4**
```python
for p in layer4.parameters():
    p.requires_grad = True
```

**ä¸ºä»€ä¹ˆåªè§£å†» layer4ï¼Ÿ**
- **æ›´ç²¾ç»†çš„å¾®è°ƒ**ï¼šlayer4 åŒ…å«é«˜çº§ç‰¹å¾ï¼Œå¯¹ä»»åŠ¡æœ€ç›¸å…³
- **å‡å°‘è¿‡æ‹Ÿåˆ**ï¼šåªè®­ç»ƒå°‘é‡å‚æ•°ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©
- **æ›´å¿«è®­ç»ƒ**ï¼šéœ€è¦æ›´æ–°çš„å‚æ•°æ›´å°‘

---

## ç¬¬äº”éƒ¨åˆ†ï¼šè¯„ä¼°å‡½æ•°ï¼ˆç¬¬149-220è¡Œï¼‰

### `EvalStats` æ•°æ®ç±»ï¼ˆç¬¬151-157è¡Œï¼‰

```python
@dataclass
class EvalStats:
    mse: float
    rmse_per_joint: np.ndarray
    action_norm_gt_mean: float
    action_norm_pred_mean: float
    cos_mean: float
```

**åŠŸèƒ½**ï¼šå­˜å‚¨è¯„ä¼°ç»Ÿè®¡ä¿¡æ¯çš„æ•°æ®ç±»ã€‚

**å­—æ®µè¯´æ˜ï¼š**
- `mse`: å¹³å‡å¹³æ–¹è¯¯å·®ï¼ˆæ‰€æœ‰ç»´åº¦çš„å¹³å‡ï¼‰
- `rmse_per_joint`: æ¯ä¸ªå…³èŠ‚çš„ RMSEï¼ˆ7 ç»´æ•°ç»„ï¼‰
- `action_norm_gt_mean`: çœŸå®åŠ¨ä½œçš„å¹³å‡èŒƒæ•°
- `action_norm_pred_mean`: é¢„æµ‹åŠ¨ä½œçš„å¹³å‡èŒƒæ•°
- `cos_mean`: å¹³å‡ä½™å¼¦ç›¸ä¼¼åº¦

---

### `evaluate(model, loader, device)`ï¼ˆç¬¬159-220è¡Œï¼‰

```python
@torch.no_grad()
def evaluate(model, loader, device) -> EvalStats:
    model.eval()
    loss_fn = nn.MSELoss(reduction="sum")

    total_mse_sum = 0.0
    total_n = 0
    # per joint
    se_sum = None  # [D]
    # norms and cosine
    gt_norms = []
    pred_norms = []
    cos_vals = []
```

**åŠŸèƒ½**ï¼šåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

**é€è¡Œè§£é‡Šï¼š**

**ç¬¬159è¡Œï¼šè£…é¥°å™¨**
```python
@torch.no_grad()
```
- **ç¦ç”¨æ¢¯åº¦è®¡ç®—**ï¼šè¯„ä¼°æ—¶ä¸éœ€è¦åå‘ä¼ æ’­ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—

**ç¬¬160è¡Œï¼šè®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼**
```python
model.eval()
```
- **å…³é—­ Dropoutã€BatchNorm çš„æ›´æ–°**ï¼šè¯„ä¼°æ—¶ä½¿ç”¨å›ºå®šçš„ç»Ÿè®¡é‡

**ç¬¬161è¡Œï¼šå®šä¹‰æŸå¤±å‡½æ•°**
```python
loss_fn = nn.MSELoss(reduction="sum")
```
- `reduction="sum"`: è¿”å›æ‰€æœ‰æ ·æœ¬çš„æŸå¤±ä¹‹å’Œï¼ˆä¸æ˜¯å¹³å‡å€¼ï¼‰
- **ä¸ºä»€ä¹ˆç”¨ sumï¼Ÿ** éœ€è¦æ‰‹åŠ¨è®¡ç®—æ€»æ ·æœ¬æ•°ï¼Œç„¶åé™¤ä»¥æ€»æ ·æœ¬æ•°

**ç¬¬163-171è¡Œï¼šåˆå§‹åŒ–ç»Ÿè®¡å˜é‡**
- `total_mse_sum`: ç´¯è®¡ MSE æ€»å’Œ
- `total_n`: æ€»æ ·æœ¬æ•°
- `se_sum`: æ¯ä¸ªå…³èŠ‚çš„å¹³æ–¹è¯¯å·®æ€»å’Œï¼ˆ`[7]` æ•°ç»„ï¼‰
- `gt_norms`: çœŸå®åŠ¨ä½œèŒƒæ•°åˆ—è¡¨
- `pred_norms`: é¢„æµ‹åŠ¨ä½œèŒƒæ•°åˆ—è¡¨
- `cos_vals`: ä½™å¼¦ç›¸ä¼¼åº¦åˆ—è¡¨

---

**ç¬¬173-198è¡Œï¼šéå†æ•°æ®åŠ è½½å™¨**

```python
pbar = tqdm(loader, desc="Val", leave=False)
for batch in pbar:
    images = batch["image"].to(device, non_blocking=True)
    target = batch["delta_q"].to(device, non_blocking=True)  # [B,7]
    pred = model(images)

    mse_sum = loss_fn(pred, target).item()
    total_mse_sum += mse_sum
    bs = images.size(0)
    total_n += bs

    # per joint squared error
    se = (pred - target) ** 2  # [B,7]
    se_batch_sum = se.sum(dim=0).detach().cpu().numpy()  # [7]
    if se_sum is None:
        se_sum = se_batch_sum
    else:
        se_sum += se_batch_sum

    # norms
    gt_norms.append(torch.norm(target, dim=1).detach().cpu().numpy())
    pred_norms.append(torch.norm(pred, dim=1).detach().cpu().numpy())
    cos_vals.append(cosine_similarity(pred, target).detach().cpu().numpy())

    # show batch mse (mean) in bar
    pbar.set_postfix(mse=f"{(mse_sum / max(bs,1)):.4f}")
```

**é€è¡Œè§£é‡Šï¼š**

**ç¬¬173è¡Œï¼šåˆ›å»ºè¿›åº¦æ¡**
```python
pbar = tqdm(loader, desc="Val", leave=False)
```
- `desc="Val"`: è¿›åº¦æ¡æè¿°
- `leave=False`: å®Œæˆåä¸ä¿ç•™è¿›åº¦æ¡

**ç¬¬175-177è¡Œï¼šåŠ è½½æ•°æ®å¹¶é¢„æµ‹**
```python
images = batch["image"].to(device, non_blocking=True)
target = batch["delta_q"].to(device, non_blocking=True)  # [B,7]
pred = model(images)
```
- `.to(device)`: å°†æ•°æ®ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
- `non_blocking=True`: **å¼‚æ­¥ä¼ è¾“**ï¼Œä¸é˜»å¡ CPU

**ç¬¬179-182è¡Œï¼šè®¡ç®—å¹¶ç´¯è®¡ MSE**
```python
mse_sum = loss_fn(pred, target).item()
total_mse_sum += mse_sum
bs = images.size(0)
total_n += bs
```
- `loss_fn(pred, target)`: è®¡ç®— MSEï¼ˆè¿”å›æ ‡é‡å¼ é‡ï¼‰
- `.item()`: å°†å¼ é‡è½¬æ¢ä¸º Python æµ®ç‚¹æ•°
- `bs = images.size(0)`: è·å– batch size
- `total_n += bs`: ç´¯è®¡æ€»æ ·æœ¬æ•°

**ç¬¬184-190è¡Œï¼šè®¡ç®—æ¯ä¸ªå…³èŠ‚çš„å¹³æ–¹è¯¯å·®**
```python
se = (pred - target) ** 2  # [B,7]
se_batch_sum = se.sum(dim=0).detach().cpu().numpy()  # [7]
if se_sum is None:
    se_sum = se_batch_sum
else:
    se_sum += se_batch_sum
```
- `(pred - target) ** 2`: é€å…ƒç´ å¹³æ–¹è¯¯å·®ï¼Œå½¢çŠ¶ `[B, 7]`
- `.sum(dim=0)`: æ²¿ batch ç»´åº¦æ±‚å’Œï¼Œå¾—åˆ°æ¯ä¸ªå…³èŠ‚çš„å¹³æ–¹è¯¯å·®æ€»å’Œï¼Œå½¢çŠ¶ `[7]`
- `.detach().cpu().numpy()`: æ–­å¼€æ¢¯åº¦ï¼Œç§»åˆ° CPUï¼Œè½¬ä¸º NumPy æ•°ç»„
- ç´¯è®¡åˆ° `se_sum`

**ç¬¬192-195è¡Œï¼šè®¡ç®—èŒƒæ•°å’Œä½™å¼¦ç›¸ä¼¼åº¦**
```python
gt_norms.append(torch.norm(target, dim=1).detach().cpu().numpy())
pred_norms.append(torch.norm(pred, dim=1).detach().cpu().numpy())
cos_vals.append(cosine_similarity(pred, target).detach().cpu().numpy())
```
- `torch.norm(..., dim=1)`: è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ L2 èŒƒæ•°ï¼Œå½¢çŠ¶ `[B]`
- å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­

**ç¬¬197-198è¡Œï¼šæ›´æ–°è¿›åº¦æ¡**
```python
pbar.set_postfix(mse=f"{(mse_sum / max(bs,1)):.4f}")
```
- æ˜¾ç¤ºå½“å‰ batch çš„å¹³å‡ MSE

---

**ç¬¬200-220è¡Œï¼šè®¡ç®—æœ€ç»ˆç»Ÿè®¡é‡**

```python
if total_n == 0:
    return EvalStats(mse=float("nan"),
                     rmse_per_joint=np.full((7,), np.nan),
                     action_norm_gt_mean=float("nan"),
                     action_norm_pred_mean=float("nan"),
                     cos_mean=float("nan"))

mse = total_mse_sum / (total_n * 7)  # average per-dim MSE
rmse_per_joint = np.sqrt(se_sum / total_n)  # [7]

gt_norms = np.concatenate(gt_norms) if len(gt_norms) else np.array([])
pred_norms = np.concatenate(pred_norms) if len(pred_norms) else np.array([])
cos_vals = np.concatenate(cos_vals) if len(cos_vals) else np.array([])

return EvalStats(
    mse=float(mse),
    rmse_per_joint=rmse_per_joint.astype(float),
    action_norm_gt_mean=float(gt_norms.mean()) if gt_norms.size else float("nan"),
    action_norm_pred_mean=float(pred_norms.mean()) if pred_norms.size else float("nan"),
    cos_mean=float(cos_vals.mean()) if cos_vals.size else float("nan"),
)
```

**é€è¡Œè§£é‡Šï¼š**

**ç¬¬200-206è¡Œï¼šå¤„ç†ç©ºæ•°æ®é›†**
- å¦‚æœæ²¡æœ‰æ ·æœ¬ï¼Œè¿”å› NaN å€¼

**ç¬¬207è¡Œï¼šè®¡ç®—å¹³å‡ MSE**
```python
mse = total_mse_sum / (total_n * 7)
```
- `total_mse_sum`: æ‰€æœ‰æ ·æœ¬çš„ MSE æ€»å’Œ
- `total_n * 7`: æ€»å…ƒç´ æ•°ï¼ˆæ ·æœ¬æ•° Ã— 7 ä¸ªå…³èŠ‚ï¼‰
- **ç»“æœ**ï¼šæ¯ä¸ªç»´åº¦çš„å¹³å‡ MSE

**ç¬¬208è¡Œï¼šè®¡ç®—æ¯ä¸ªå…³èŠ‚çš„ RMSE**
```python
rmse_per_joint = np.sqrt(se_sum / total_n)  # [7]
```
- `se_sum / total_n`: æ¯ä¸ªå…³èŠ‚çš„å¹³å‡å¹³æ–¹è¯¯å·®
- `np.sqrt(...)`: å¼€å¹³æ–¹ï¼Œå¾—åˆ° RMSE
- **ç»“æœ**ï¼š`[7]` æ•°ç»„ï¼Œæ¯ä¸ªå…³èŠ‚çš„ RMSE

**ç¬¬210-212è¡Œï¼šåˆå¹¶åˆ—è¡¨**
```python
gt_norms = np.concatenate(gt_norms) if len(gt_norms) else np.array([])
pred_norms = np.concatenate(pred_norms) if len(pred_norms) else np.array([])
cos_vals = np.concatenate(cos_vals) if len(cos_vals) else np.array([])
```
- å°†å¤šä¸ª batch çš„ç»“æœåˆå¹¶ä¸ºä¸€ä¸ªæ•°ç»„

**ç¬¬214-220è¡Œï¼šè¿”å›ç»Ÿè®¡ç»“æœ**
- è®¡ç®—å¹³å‡å€¼å¹¶è¿”å› `EvalStats` å¯¹è±¡

---

## ç¬¬å…­éƒ¨åˆ†ï¼šè®­ç»ƒå‡½æ•°ï¼ˆç¬¬222-247è¡Œï¼‰

### `train_one_epoch(model, loader, optimizer, device, grad_clip)`ï¼ˆç¬¬222-247è¡Œï¼‰

```python
def train_one_epoch(model, loader, optimizer, device, grad_clip: float = 0.0) -> float:
    model.train()
    loss_fn = nn.MSELoss()
    total_loss = 0.0
    n = 0

    pbar = tqdm(loader, desc="Train", leave=False)
    for batch in pbar:
        images = batch["image"].to(device, non_blocking=True)
        target = batch["delta_q"].to(device, non_blocking=True)

        pred = model(images)
        loss = loss_fn(pred, target)

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        if grad_clip and grad_clip > 0:
            nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()

        bs = images.size(0)
        total_loss += loss.item() * bs
        n += bs
        pbar.set_postfix(loss=f"{loss.item():.4f}")

    return total_loss / max(n, 1)
```

**åŠŸèƒ½**ï¼šè®­ç»ƒä¸€ä¸ª epochã€‚

**é€è¡Œè§£é‡Šï¼š**

**ç¬¬223è¡Œï¼šè®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼**
```python
model.train()
```
- **å¯ç”¨ Dropoutã€BatchNorm çš„æ›´æ–°**

**ç¬¬224è¡Œï¼šå®šä¹‰æŸå¤±å‡½æ•°**
```python
loss_fn = nn.MSELoss()
```
- é»˜è®¤ `reduction="mean"`ï¼ˆè¿”å›å¹³å‡å€¼ï¼‰

**ç¬¬225-226è¡Œï¼šåˆå§‹åŒ–ç»Ÿè®¡å˜é‡**
```python
total_loss = 0.0
n = 0
```

**ç¬¬228-245è¡Œï¼šè®­ç»ƒå¾ªç¯**

**ç¬¬230-231è¡Œï¼šåŠ è½½æ•°æ®**
```python
images = batch["image"].to(device, non_blocking=True)
target = batch["delta_q"].to(device, non_blocking=True)
```

**ç¬¬233-234è¡Œï¼šå‰å‘ä¼ æ’­**
```python
pred = model(images)
loss = loss_fn(pred, target)
```

**ç¬¬236-240è¡Œï¼šåå‘ä¼ æ’­å’Œä¼˜åŒ–**
```python
optimizer.zero_grad(set_to_none=True)
loss.backward()
if grad_clip and grad_clip > 0:
    nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
optimizer.step()
```
- **`optimizer.zero_grad(set_to_none=True)`**: æ¸…é›¶æ¢¯åº¦
  - `set_to_none=True`: å°†æ¢¯åº¦è®¾ä¸º `None` è€Œä¸æ˜¯ 0ï¼ˆæ›´é«˜æ•ˆï¼‰
- **`loss.backward()`**: åå‘ä¼ æ’­ï¼Œè®¡ç®—æ¢¯åº¦
- **`nn.utils.clip_grad_norm_(..., grad_clip)`**: **æ¢¯åº¦è£å‰ª**ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
  - å¦‚æœæ¢¯åº¦èŒƒæ•°è¶…è¿‡ `grad_clip`ï¼Œå°†å…¶ç¼©æ”¾
- **`optimizer.step()`**: æ›´æ–°å‚æ•°

**ç¬¬242-245è¡Œï¼šç´¯è®¡æŸå¤±**
```python
bs = images.size(0)
total_loss += loss.item() * bs
n += bs
pbar.set_postfix(loss=f"{loss.item():.4f}")
```
- `loss.item() * bs`: å°†å¹³å‡æŸå¤±è½¬æ¢ä¸ºæ€»æŸå¤±ï¼ˆå› ä¸º `loss` æ˜¯å¹³å‡å€¼ï¼‰
- ç´¯è®¡æ€»æŸå¤±å’Œæ ·æœ¬æ•°

**ç¬¬247è¡Œï¼šè¿”å›å¹³å‡æŸå¤±**
```python
return total_loss / max(n, 1)
```

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ—©åœï¼ˆEarly Stoppingï¼‰ï¼ˆç¬¬250-267è¡Œï¼‰

### `EarlyStopper` ç±»ï¼ˆç¬¬252-266è¡Œï¼‰

```python
class EarlyStopper:
    def __init__(self, patience: int = 10, min_delta: float = 1e-5):
        self.patience = patience
        self.min_delta = min_delta
        self.best = float("inf")
        self.bad_epochs = 0

    def step(self, val: float) -> bool:
        # returns True if should stop
        if val + self.min_delta < self.best:
            self.best = val
            self.bad_epochs = 0
            return False
        self.bad_epochs += 1
        return self.bad_epochs >= self.patience
```

**åŠŸèƒ½**ï¼šå®ç°æ—©åœæœºåˆ¶ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

**é€è¡Œè§£é‡Šï¼š**

**`__init__` æ–¹æ³•ï¼š**
- `patience`: **å®¹å¿åº¦**ï¼Œè¿ç»­å¤šå°‘ä¸ª epoch æ²¡æœ‰æ”¹å–„å°±åœæ­¢
- `min_delta`: **æœ€å°æ”¹å–„é‡**ï¼Œåªæœ‰æ”¹å–„è¶…è¿‡è¿™ä¸ªå€¼æ‰è®¤ä¸ºæ˜¯çœŸæ­£çš„æ”¹å–„
- `best`: æœ€ä½³éªŒè¯å€¼ï¼ˆåˆå§‹ä¸ºæ— ç©·å¤§ï¼‰
- `bad_epochs`: è¿ç»­æ²¡æœ‰æ”¹å–„çš„ epoch æ•°

**`step` æ–¹æ³•ï¼š**
- **è¾“å…¥**ï¼šå½“å‰éªŒè¯å€¼ `val`
- **è¿”å›**ï¼š`True` è¡¨ç¤ºåº”è¯¥åœæ­¢ï¼Œ`False` è¡¨ç¤ºç»§ç»­

**é€»è¾‘ï¼š**
1. å¦‚æœ `val + min_delta < self.best`ï¼ˆæœ‰æ˜¾è‘—æ”¹å–„ï¼‰ï¼š
   - æ›´æ–° `best`
   - é‡ç½® `bad_epochs = 0`
   - è¿”å› `False`ï¼ˆç»§ç»­è®­ç»ƒï¼‰
2. å¦åˆ™ï¼ˆæ²¡æœ‰æ”¹å–„ï¼‰ï¼š
   - `bad_epochs += 1`
   - å¦‚æœ `bad_epochs >= patience`ï¼Œè¿”å› `True`ï¼ˆåœæ­¢è®­ç»ƒï¼‰

**ä¸ºä»€ä¹ˆéœ€è¦ `min_delta`ï¼Ÿ**
- é¿å…å› ä¸ºå¾®å°çš„éšæœºæ³¢åŠ¨è€Œé‡ç½®è®¡æ•°å™¨

---

## ç¬¬å…«éƒ¨åˆ†ï¼šä¸»å‡½æ•°ï¼ˆç¬¬269-489è¡Œï¼‰

### å‚æ•°è§£æï¼ˆç¬¬272-288è¡Œï¼‰

```python
def main():
    parser = argparse.ArgumentParser(description="Managed BC Training (ResNet18 + MLP)")
    parser.add_argument("--dataset_root", type=str, default="/home/alphatok/ME5400/expert_data")
    parser.add_argument("--out_dir", type=str, default="./checkpoints_bc_managed")
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--num_workers", type=int, default=4)
    parser.add_argument("--lr_head", type=float, default=1e-3, help="LR for head-only phase")
    parser.add_argument("--lr_finetune", type=float, default=1e-4, help="LR for finetune phase")
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--freeze_epochs", type=int, default=8, help="epochs to train head only")
    parser.add_argument("--unfreeze_layer4", action="store_true", help="finetune only layer4 instead of full backbone")
    parser.add_argument("--image_height", type=int, default=240)
    parser.add_argument("--image_width", type=int, default=320)
    parser.add_argument("--only_success", action="store_true")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--patience", type=int, default=10)
    parser.add_argument("--grad_clip", type=float, default=1.0)
    args = parser.parse_args()
```

**å‚æ•°è¯´æ˜ï¼š**
- `--dataset_root`: æ•°æ®é›†æ ¹ç›®å½•
- `--out_dir`: è¾“å‡ºç›®å½•ï¼ˆä¿å­˜ checkpoints å’ŒæŒ‡æ ‡ï¼‰
- `--batch_size`: batch å¤§å°ï¼ˆé»˜è®¤ 32ï¼‰
- `--num_workers`: æ•°æ®åŠ è½½çš„å¹¶è¡Œè¿›ç¨‹æ•°
- `--lr_head`: head-only é˜¶æ®µçš„å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 1e-3ï¼Œè¾ƒå¤§ï¼‰
- `--lr_finetune`: finetune é˜¶æ®µçš„å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 1e-4ï¼Œè¾ƒå°ï¼‰
- `--epochs`: æœ€å¤§è®­ç»ƒ epoch æ•°
- `--freeze_epochs`: åªè®­ç»ƒ head çš„ epoch æ•°ï¼ˆé»˜è®¤ 8ï¼‰
- `--unfreeze_layer4`: å¦‚æœè®¾ç½®ï¼Œåªè§£å†» layer4ï¼ˆå¦åˆ™è§£å†»æ•´ä¸ª backboneï¼‰
- `--image_height/width`: å›¾åƒå°ºå¯¸
- `--only_success`: å¦‚æœè®¾ç½®ï¼Œåªä½¿ç”¨æˆåŠŸçš„ episode
- `--seed`: éšæœºç§å­
- `--patience`: æ—©åœçš„å®¹å¿åº¦
- `--grad_clip`: æ¢¯åº¦è£å‰ªé˜ˆå€¼

---

### åˆå§‹åŒ–ï¼ˆç¬¬290-308è¡Œï¼‰

```python
set_seed(args.seed)
ensure_dir(args.out_dir)
plots_dir = os.path.join(args.out_dir, "plots")
ensure_dir(plots_dir)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

image_size_hw = (args.image_height, args.image_width)

# Data
train_set, val_set, train_loader, val_loader = make_loaders(
    args.dataset_root,
    batch_size=args.batch_size,
    num_workers=args.num_workers,
    image_size_hw=image_size_hw,
    only_success=args.only_success,
)
print(f"Train samples: {len(train_set)} | Val samples: {len(val_set)}")
```

**é€è¡Œè§£é‡Šï¼š**
- è®¾ç½®éšæœºç§å­
- åˆ›å»ºè¾“å‡ºç›®å½•å’Œ plots ç›®å½•
- é€‰æ‹©è®¾å¤‡ï¼ˆGPU æˆ– CPUï¼‰
- åˆ›å»ºæ•°æ®åŠ è½½å™¨
- æ‰“å°æ•°æ®é›†å¤§å°

---

### æ¨¡å‹å’Œä¼˜åŒ–å™¨åˆå§‹åŒ–ï¼ˆç¬¬310-318è¡Œï¼‰

```python
# Model
model = ResNetMLPPolicy(out_dim=7).to(device)

# Phase 1: freeze backbone, train head
freeze_backbone(model, freeze=True)
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr_head)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=3)

early = EarlyStopper(patience=args.patience, min_delta=1e-6)
```

**é€è¡Œè§£é‡Šï¼š**
- åˆ›å»ºæ¨¡å‹å¹¶ç§»åˆ°è®¾å¤‡
- **å†»ç»“ backbone**ï¼Œåªè®­ç»ƒ head
- **åˆ›å»ºä¼˜åŒ–å™¨**ï¼šåªä¼˜åŒ–éœ€è¦æ¢¯åº¦çš„å‚æ•°ï¼ˆ`filter(lambda p: p.requires_grad, ...)`ï¼‰
- **åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼š`ReduceLROnPlateau`
  - `mode="min"`: ç›‘æ§éªŒè¯ lossï¼Œå½“ loss ä¸å†ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡
  - `factor=0.5`: æ¯æ¬¡é™ä½ä¸ºåŸæ¥çš„ 0.5 å€
  - `patience=3`: è¿ç»­ 3 ä¸ª epoch æ²¡æœ‰æ”¹å–„å°±é™ä½å­¦ä¹ ç‡
- åˆ›å»ºæ—©åœå™¨

---

### æ—¥å¿—åˆå§‹åŒ–ï¼ˆç¬¬320-332è¡Œï¼‰

```python
# Logging
metrics_csv = os.path.join(args.out_dir, "metrics.csv")
metrics_jsonl = os.path.join(args.out_dir, "metrics.jsonl")
with open(metrics_csv, "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerow([
        "epoch", "phase", "train_loss",
        "val_mse", "val_cos", "val_norm_gt", "val_norm_pred",
        "lr"
    ] + [f"val_rmse_j{i+1}" for i in range(7)])
# also dump config
with open(os.path.join(args.out_dir, "run_args.json"), "w") as f:
    json.dump(vars(args), f, indent=2)
```

**åŠŸèƒ½**ï¼šåˆå§‹åŒ– CSV å’Œ JSONL æ—¥å¿—æ–‡ä»¶ï¼Œä¿å­˜è¿è¡Œé…ç½®ã€‚

---

### è®­ç»ƒå¾ªç¯ï¼ˆç¬¬352-481è¡Œï¼‰

```python
for epoch in range(args.epochs):
    # switch phase
    if epoch < args.freeze_epochs:
        phase = "head_only"
    else:
        phase = "finetune"
    
    # åœ¨åˆ‡æ¢åˆ°finetuneé˜¶æ®µæ—¶ï¼Œè§£å†»backboneå¹¶é‡æ–°åˆå§‹åŒ–optimizer
    if epoch == args.freeze_epochs:
        # unfreeze
        if args.unfreeze_layer4:
            unfreeze_layer4_only(model)
            print("âœ… finetune: ä»…è§£å†» layer4")
        else:
            freeze_backbone(model, freeze=False)
            print("âœ… finetune: è§£å†»æ•´ä¸ª backbone")

        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr_finetune)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode="min", factor=0.5, patience=3)
        early = EarlyStopper(patience=args.patience, min_delta=1e-6)

    # train
    train_loss = train_one_epoch(model, train_loader, optimizer, device, grad_clip=args.grad_clip)
    # val stats
    val_stats = evaluate(model, val_loader, device)

    lr = optimizer.param_groups[0]["lr"]
    print(
        f"Epoch {epoch:03d} [{phase}] "
        f"train_loss={train_loss:.6f} | val_mse={val_stats.mse:.6f} "
        f"| cos={val_stats.cos_mean:.3f} | lr={lr:.2e}"
    )

    # scheduler uses val mse
    scheduler.step(val_stats.mse)

    # save best
    if val_stats.mse < best_val:
        best_val = val_stats.mse
        torch.save(
            {
                "model": model.state_dict(),
                "epoch": epoch,
                "val_mse": val_stats.mse,
                "args": vars(args),
            },
            best_path,
        )
        print(f"  [saved] {best_path}")
```

**é€è¡Œè§£é‡Šï¼š**

**ç¬¬354-357è¡Œï¼šç¡®å®šå½“å‰é˜¶æ®µ**
- å‰ `freeze_epochs` ä¸ª epoch æ˜¯ "head_only" é˜¶æ®µ
- ä¹‹åæ˜¯ "finetune" é˜¶æ®µ

**ç¬¬359-371è¡Œï¼šåˆ‡æ¢åˆ° finetune é˜¶æ®µ**
- å½“ `epoch == args.freeze_epochs` æ—¶ï¼š
  - è§£å†» backboneï¼ˆæˆ–åªè§£å†» layer4ï¼‰
  - **é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨**ï¼ˆå› ä¸ºå¯è®­ç»ƒå‚æ•°å˜äº†ï¼‰
  - **é‡æ–°åˆ›å»ºè°ƒåº¦å™¨å’Œæ—©åœå™¨**ï¼ˆé‡ç½®çŠ¶æ€ï¼‰

**ç¬¬373-376è¡Œï¼šè®­ç»ƒå’ŒéªŒè¯**
- è®­ç»ƒä¸€ä¸ª epoch
- åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°

**ç¬¬378-383è¡Œï¼šæ‰“å°ä¿¡æ¯**
- æ˜¾ç¤ºå½“å‰ epochã€é˜¶æ®µã€lossã€ä½™å¼¦ç›¸ä¼¼åº¦ã€å­¦ä¹ ç‡

**ç¬¬385è¡Œï¼šæ›´æ–°å­¦ä¹ ç‡**
```python
scheduler.step(val_stats.mse)
```
- æ ¹æ®éªŒè¯ MSE è°ƒæ•´å­¦ä¹ ç‡

**ç¬¬387-400è¡Œï¼šä¿å­˜æœ€ä½³æ¨¡å‹**
- å¦‚æœå½“å‰éªŒè¯ MSE æ›´å¥½ï¼Œä¿å­˜ checkpoint

**ç¬¬402-424è¡Œï¼šè®°å½•æŒ‡æ ‡**
- å†™å…¥ CSV å’Œ JSONL æ–‡ä»¶

**ç¬¬426-476è¡Œï¼šæ›´æ–°å†å²å¹¶ç»˜åˆ¶å›¾è¡¨**
- æ›´æ–°å†å²è®°å½•
- ç»˜åˆ¶ loss æ›²çº¿ã€æ¯ä¸ªå…³èŠ‚çš„ RMSEã€ä½™å¼¦ç›¸ä¼¼åº¦ã€åŠ¨ä½œèŒƒæ•°

**ç¬¬478-481è¡Œï¼šæ—©åœæ£€æŸ¥**
```python
if early.step(val_stats.mse):
    print(f"ğŸ›‘ Early stopping triggered at epoch {epoch} (best val_mse={early.best:.6f})")
    break
```
- å¦‚æœæ—©åœå™¨è¿”å› `True`ï¼Œåœæ­¢è®­ç»ƒ

---

## æ€»ç»“

è¿™ä¸ªè®­ç»ƒè„šæœ¬å®ç°äº†ï¼š

1. **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼šå…ˆè®­ç»ƒ headï¼Œå†å¾®è°ƒ backbone
2. **å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡**ï¼šMSEã€RMSEã€ä½™å¼¦ç›¸ä¼¼åº¦ã€åŠ¨ä½œèŒƒæ•°
3. **è‡ªåŠ¨å­¦ä¹ ç‡è°ƒæ•´**ï¼š`ReduceLROnPlateau`
4. **æ—©åœæœºåˆ¶**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
5. **è¯¦ç»†çš„æ—¥å¿—è®°å½•**ï¼šCSVã€JSONLã€å¯è§†åŒ–å›¾è¡¨
6. **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

è¿™æ˜¯ä¸€ä¸ª**ç”Ÿäº§çº§åˆ«çš„è®­ç»ƒè„šæœ¬**ï¼Œé€‚åˆå®é™…é¡¹ç›®ä½¿ç”¨ã€‚

